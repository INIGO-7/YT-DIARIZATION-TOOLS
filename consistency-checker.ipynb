{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path = \"diarized_audio/raw_diarizations\"\n",
    "diarization_paths = {}\n",
    "for directory in os.listdir(d_path):\n",
    "    diarization_paths[directory] = os.path.join(d_path, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Careful!` -- the following cell deletes previous work. If intended, run to make way for the new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'diarized_audio/raw_diarizations/lexFridman-mrBeast-11012023/speaker_turns.csv': No such file or directory\n",
      "rm: cannot remove 'diarized_audio/raw_diarizations/flagrant-joeRogan-07062022/speaker_turns.csv': No such file or directory\n",
      "rm: cannot remove 'diarized_audio/raw_diarizations/JRE-wizKhalifa-31032019/speaker_turns.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# # Remove all csv files from diarization directory\n",
    "for path in os.listdir(d_path):\n",
    "    !rm -r {os.path.join(d_path, path)}/speaker_turns.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_directories(arr):\n",
    "    def sort_key(s):\n",
    "        return int(s.split('_')[1])\n",
    "\n",
    "    return sorted(arr, key=sort_key)\n",
    "\n",
    "def parse_time(time_str):\n",
    "    # Parses the time string to a pandas.Timedelta object\n",
    "    return pd.Timedelta(time_str)\n",
    "\n",
    "def process_line(line):\n",
    "    # Splits the line and extracts the required information\n",
    "    time_data, speaker = line.split(']')\n",
    "    time_data = time_data[2:]\n",
    "    start_time, end_time = time_data.split(' -->  ')\n",
    "    speaker = speaker.split(' ')[-1]  # Gets only the SPEAKER_xx part\n",
    "    return speaker, parse_time(start_time), parse_time(end_time)\n",
    "\n",
    "def most_common_letters(string, dictionary):\n",
    "    def common_letter_count(s1, s2):\n",
    "        return sum(min(s1.count(c), s2.count(c)) for c in set(s1))\n",
    "\n",
    "    max_key = max(dictionary.keys(), key=lambda k: common_letter_count(k.lower(), string.lower()))\n",
    "    return dictionary[max_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_turns(podcast_dictionaries, verbose : bool = False):\n",
    "    diarizations = \"diarized_audio/raw_diarizations\"\n",
    "    directories = [\n",
    "        directory \n",
    "        for directory in os.listdir(diarizations) \n",
    "        if directory in podcast_dictionaries.keys()\n",
    "    ]\n",
    "    \n",
    "    for podcast_name in directories:\n",
    "\n",
    "        directory = os.path.join(diarizations, podcast_name)\n",
    "        podcast_df = pd.DataFrame(columns=['speaker', 'start', 'end'])\n",
    "        spacer_time = pd.Timedelta('0 days 00:00:00.998000')\n",
    "        global_time = pd.Timedelta('0')\n",
    "        splits = sort_directories([d for d in os.listdir(directory) if d.endswith('.txt')])\n",
    "        #to_replace_dict = most_common_letters(podcast_name, podcast_dictionaries)\n",
    "        to_replace_dict = podcast_dictionaries[podcast_name]\n",
    "        \n",
    "        if verbose: \n",
    "            print(directory)\n",
    "            print(to_replace_dict)\n",
    "        \n",
    "        for idx, split in enumerate(splits):\n",
    "            \n",
    "            file_path = os.path.join(directory, split)\n",
    "            # get split number with regex that searches for a number\n",
    "            number = re.search(r'\\d+(\\.\\d+)?', split).group()\n",
    "            # get the speakers (SPEAKER_00, SPEAKER_...) that talk in this split\n",
    "\n",
    "            # In case we want to discard the data in one of the splits, just go to the next split.\n",
    "            if to_replace_dict[number] == \"\":\n",
    "                continue\n",
    "\n",
    "            real_speakers = to_replace_dict[number].split(\";\")\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    speaker, start, end = process_line(line)\n",
    "\n",
    "                    # Set the correct speaker\n",
    "\n",
    "                    speaker = speaker.strip('\\n')\n",
    "                    for rsp in real_speakers:\n",
    "                        rsp = rsp.split(\",\")\n",
    "                        pyannote_speaker = 'SPEAKER_' + rsp[0]\n",
    "                        \n",
    "                        if pyannote_speaker == speaker:\n",
    "                            speaker = rsp[1]\n",
    "                    \n",
    "                    # Adjust start and end to the global time\n",
    "                    start = start + global_time - spacer_time\n",
    "                    end = end + global_time - spacer_time\n",
    "\n",
    "                    #Create a new tagged speaker intervention in the podcast\n",
    "                    podcast_df.loc[len(podcast_df)] = [speaker, start, end]\n",
    "            \n",
    "            global_time = podcast_df[\"end\"].max()\n",
    "    \n",
    "        pattern = 'SPEAKER_\\d+'\n",
    "        filtered_df = podcast_df[~podcast_df['speaker'].str.contains(pattern, na=False)]\n",
    "        filtered_df.to_csv(os.path.join(directory, \"speaker_turns.csv\"), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diarized_audio/raw_diarizations/lexFridman-guidoVanRossum-26112022\n",
      "{'1': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '2': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '3': '00,LEXFRIDMAN;01,GUIDOVANROSSUM;02,LEXFRIDMAN', '4': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '5': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '6': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '7': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '8': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '9': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '10': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '11': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '12': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '13': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '14': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '15': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '16': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '17': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '18': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '19': '00,LEXFRIDMAN;01,GUIDOVANROSSUM'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diarized_audio/raw_diarizations/lexFridman-markZuckerberg-09062023\n",
      "{'1': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '2': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '4': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '5': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '6': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '7': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '8': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '9': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '10': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '11': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '12': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '13': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '14': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '15': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '16': '00,LEXFRIDMAN;01,MARKZUCKERBERG'}\n",
      "diarized_audio/raw_diarizations/lexFridman-matthewMcConaughey-13062023\n",
      "{'1': '00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN', '2': '00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN', '3': '00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN', '4': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '5': '00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN', '6': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '7': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '8': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '9': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '10': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '11': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '12': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '13': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '14': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY'}\n",
      "diarized_audio/raw_diarizations/JRE-neildeGrasseTyson-06092019\n",
      "{'1': '00,NILDEGRASSE;01,JOEROGAN', '2': '00,NILDEGRASSE;01,JOEROGAN', '3': '00,NILDEGRASSE;01,NILDEGRASSE;02,NILDEGRASSE', '4': '00,NILDEGRASSE;01,JOEROGAN', '5': '00,JOEROGAN;01,NILDEGRASSE', '6': '00,JOEROGAN;01,NILDEGRASSE', '7': '00,NILDEGRASSE;01,JOEROGAN', '8': '00,NILDEGRASSE;01,JOEROGAN', '9': '00,JOEROGAN;01,NILDEGRASSE', '10': '00,JOEROGAN;01,NILDEGRASSE', '11': '00,NILDEGRASSE;01,JOEROGAN', '12': '00,NILDEGRASSE;01,JOEROGAN', '13': '00,NILDEGRASSE;01,JOEROGAN', '14': '00,NILDEGRASSE;01,NILDEGRASSE;02,JOEROGAN'}\n",
      "diarized_audio/raw_diarizations/JRE-edwardSnowden-23102019\n",
      "{'1': '00,SNOWDEN;01,JOEROGAN', '2': '00,SNOWDEN', '3': '00,SNOWDEN;01,JOEROGAN', '4': '00,SNOWDEN;01,JOEROGAN', '5': '01,SNOWDEN;00,JOEROGAN', '6': '00,SNOWDEN;01,JOEROGAN', '7': '00,SNOWDEN', '8': '00,SNOWDEN', '9': '01,SNOWDEN;00,JOEROGAN', '10': '01,SNOWDEN;00,JOEROGAN', '11': '00,SNOWDEN', '12': '01,SNOWDEN;00,JOEROGAN', '13': '00,SNOWDEN;01,JOEROGAN', '14': '00,SNOWDEN;01,JOEROGAN', '15': '00,SNOWDEN', '16': '01,SNOWDEN;00,JOEROGAN', '17': '00,SNOWDEN;01,JOEROGAN'}\n",
      "diarized_audio/raw_diarizations/lexFridman-andrewHuberman-17082023\n",
      "{'1': '00,ANDREWHUBERMAN;01,LEXFRIDMAN', '2': '00,ANDREWHUBERMAN;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '4': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '5': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '6': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '7': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '8': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '9': '00,ANDREWHUBERMAN;01,LEXFRIDMAN', '10': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '11': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '12': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '13': '00,LEXFRIDMAN;01,ANDREWHUBERMAN'}\n",
      "diarized_audio/raw_diarizations/JRE-mikeTyson-04092020\n",
      "{'1': '00,JOEROGAN;01,MIKETYSON', '2': '00,MIKETYSON;01,JOEROGAN', '3': '00,MIKETYSON;01,JOEROGAN', '4': '00,JOEROGAN;01,MIKETYSON', '5': '00,MIKETYSON;01,JOEROGAN', '6': '00,MIKETYSON;01,JOEROGAN', '7': '00,MIKETYSON;01,JOEROGAN', '8': '00,JOEROGAN;01,MIKETYSON', '9': '00,JOEROGAN;01,MIKETYSON', '10': '00,JOEROGAN;01,MIKETYSON', '11': '00,JOEROGAN;01,MIKETYSON', '12': '00,MIKETYSON;01,JOEROGAN'}\n",
      "diarized_audio/raw_diarizations/flagrant-grahamHancock-27062023\n",
      "{'1': '01,HANCOCK;00,ANDREW', '2': '01,HANCOCK;00,ANDREW', '3': '03,HANCOCK;02,ANDREW', '4': '01,HANCOCK;02,ANDREW', '5': '01,HANCOCK;02,ANDREW', '6': '00,HANCOCK', '7': '01,HANCOCK', '8': '02,HANCOCK;01,ANDREW', '9': '01,HANCOCK;02,ANDREW', '10': '00,HANCOCK;01,ANDREW', '11': '00,HANCOCK;01,ANDREW', '12': '00,HANCOCK;01,ANDREW', '13': '02,HANCOCK', '14': '02,HANCOCK;01,ANDREW', '15': '03,HANCOCK;00,ANDREW', '16': '', '17': '00,HANCOCK;02,ANDREW', '18': '00,HANCOCK'}\n",
      "diarized_audio/raw_diarizations/JRE-postMalone-29072020\n",
      "{'1': '00,POSTMALONE;01,JOEROGAN', '2': '00,POSTMALONE;01,JOEROGAN', '3': '00,POSTMALONE;01,JOEROGAN', '4': '00,JOEROGAN;01,POSTMALONE', '5': '00,POSTMALONE;01,JOEROGAN', '6': '00,JOEROGAN;01,POSTMALONE', '7': '00,JOEROGAN;01,POSTMALONE', '8': '', '9': '00,JOEROGAN;01,POSTMALONE', '10': '00,JOEROGAN;01,POSTMALONE', '11': '', '12': '00,JOEROGAN;01,POSTMALONE', '13': '01,POSTMALONE;02,JOEROGAN', '14': '00,POSTMALONE;01,JOEROGAN;02,POSTMALONE', '15': '00,POSTMALONE;01,POSTMALONE;02,JOEROGAN', '16': '00,JOEROGAN;01,POSTMALONE', '17': '00,POSTMALONE;01,JOEROGAN;02,POSTMALONE', '18': '00,JOEROGAN;01,POSTMALONE', '19': '00,POSTMALONE;01,JOEROGAN', '20': '01,POSTMALONE;02,JOEROGAN', '21': '', '22': '00,POSTMALONE;01,POSTMALONE;02,JOEROGAN', '23': '00,POSTMALONE;01,JOEROGAN'}\n",
      "diarized_audio/raw_diarizations/JRE-kanyeWest-24102020\n",
      "{'1': '', '2': '00,KANYE;01,JOEROGAN', '3': '01,KANYE', '4': '00,KANYE;01,JOEROGAN', '5': '01,KANYE;00,JOEROGAN', '6': '01,KANYE;00,JOEROGAN', '7': '01,KANYE;00,JOEROGAN', '8': '01,KANYE;00,JOEROGAN', '9': '00,KANYE;01,JOEROGAN', '10': '01,KANYE;00,JOEROGAN', '11': '02,KANYE;00,JOEROGAN', '12': '00,KANYE', '13': '01,KANYE;00,JOEROGAN', '14': '01,KANYE;00,JOEROGAN', '15': '00,KANYE;01,JOEROGAN', '16': '02,KANYE;00,JOEROGAN', '17': '01,KANYE;00,JOEROGAN', '18': '02,KANYE;00,JOEROGAN'}\n",
      "diarized_audio/raw_diarizations/JRE-kevinHart-25052020\n",
      "{'1': '00,KEVINHART;01,JOEROGAN', '2': '00,KEVINHART;01,JOEROGAN', '3': '00,KEVINHART;01,JOEROGAN', '4': '00,JOEROGAN;01,KEVINHART', '5': '00,JOEROGAN;01,KEVINHART', '6': '00,KEVINHART;01,JOEROGAN', '7': '00,KEVINHART;01,JOEROGAN', '8': '00,KEVINHART;01,JOEROGAN', '9': '01,KEVINHART;02,JOEROGAN', '10': '00,KEVINHART;01,JOEROGAN', '11': '00,KEVINHART;01,JOEROGAN', '12': '00,KEVINHART;01,JOEROGAN'}\n",
      "diarized_audio/raw_diarizations/JRE-joeyDiaz-26032020\n",
      "{'1': '01,JOEY;02,JOEROGAN', '2': '00,JOEY;01,JOEROGAN', '3': '00,JOEY;01,JOEROGAN', '4': '01,JOEY;00,JOEROGAN', '5': '02,JOEY;01,JOEROGAN', '6': '02,JOEY;03,JOEROGAN', '7': '00,JOEY;01,JOEROGAN', '8': '00,JOEY;01,JOEROGAN', '9': '01,JOEY;00,JOEROGAN', '10': '00,JOEY;01,JOEROGAN', '11': '00,JOEY;02,JOEROGAN', '12': '01,JOEY;00,JOEROGAN', '13': '01,JOEY;02,JOEROGAN', '14': '02,JOEY;00,JOEROGAN', '15': '01,JOEY;00,JOEROGAN', '16': '00,JOEY;01,JOEROGAN', '17': '00,JOEY', '18': '00,JOEY;01,JOEROGAN'}\n",
      "diarized_audio/raw_diarizations/JRE-mileyCirus-02092020\n",
      "{'1': '00,JOEROGAN;01,MILEYCYRUS', '2': '00,JOEROGAN;01,MILEYCYRUS', '3': '00,MILEYCYRUS;01,JOEROGAN', '4': '00,MILEYCYRUS;01,JOEROGAN', '5': '00,MILEYCYRUS;01,JOEROGAN', '6': '00,MILEYCYRUS;01,MILEYCYRUS;02,MILEYCYRUS;03,JOEROGAN', '7': '00,MILEYCYRUS;01,JOEROGAN', '8': '00,JOEROGAN;01,MILEYCYRUS', '9': '00,MILEYCYRUS;01,JOEROGAN', '10': '00,JOEROGAN;01,MILEYCYRUS', '11': '00,JOEROGAN;01,MILEYCYRUS', '12': '00,MILEYCYRUS;01,JOEROGAN', '13': '00,JOEROGAN;01,MILEYCYRUS'}\n",
      "diarized_audio/raw_diarizations/lexFridman-kanyeWest-24102022\n",
      "{'1': '00,LEXFRIDMAN;01,KANYEWEST', '2': '00,KANYEWEST;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,KANYEWEST', '4': '00,LEXFRIDMAN;01,KANYEWEST', '5': '00,KANYEWEST;01,LEXFRIDMAN', '6': '00,LEXFRIDMAN;01,KANYEWEST', '7': '00,KANYEWEST;01,LEXFRIDMAN', '8': '00,KANYEWEST;01,LEXFRIDMAN', '9': '00,KANYEWEST;01,LEXFRIDMAN', '10': '00,LEXFRIDMAN;01,KANYEWEST', '11': '00,LEXFRIDMAN;01,KANYEWEST', '12': '00,LEXFRIDMAN;01,KANYEWEST', '13': '00,KANYEWEST;01,LEXFRIDMAN', '14': '00,LEXFRIDMAN;01,KANYEWEST', '15': '00,KANYEWEST;01,LEXFRIDMAN'}\n",
      "diarized_audio/raw_diarizations/lexFridman-markZuckerberg-28092023\n",
      "{'1': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '2': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '4': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '5': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '6': '00,MARKZUCKERBERG;01,LEXFRIDMAN'}\n",
      "diarized_audio/raw_diarizations/lexFridman-benShapiro-07112022\n",
      "{'1': '00,BENSHAPIRO;01,LEXFRIDMAN', '2': '00,BENSHAPIRO;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,BENSHAPIRO', '4': '00,BENSHAPIRO;01,LEXFRIDMAN', '5': '00,LEXFRIDMAN;01,BENSHAPIRO', '6': '00,BENSHAPIRO;01,LEXFRIDMAN', '7': '00,BENSHAPIRO;01,LEXFRIDMAN', '8': '00,BENSHAPIRO;01,LEXFRIDMAN', '9': '00,BENSHAPIRO;01,LEXFRIDMAN', '10': '00,BENSHAPIRO;01,LEXFRIDMAN', '11': '00,BENSHAPIRO;01,LEXFRIDMAN', '12': '00,BENSHAPIRO;01,LEXFRIDMAN', '13': '00,LEXFRIDMAN;01,BENSHAPIRO', '14': '00,LEXFRIDMAN;01,BENSHAPIRO', '15': '00,LEXFRIDMAN;01,BENSHAPIRO'}\n",
      "diarized_audio/raw_diarizations/flagrant-mrBeast-27092022\n",
      "{'1': '00,MRBEAST', '2': '00,MRBEAST;03,ANDREW', '3': '01,MRBEAST', '4': '00,MRBEAST', '5': '01,MRBEAST;00,ANDREW', '6': '01,MRBEAST;00,ANDREW', '7': '00,MRBEAST;01,ANDREW', '8': '03,MRBEAST;02,ANDREW', '9': '00,MRBEAST;01,ANDREW', '10': '02,MRBEAST', '11': '02,MRBEAST;01,ANDREW', '12': '00,MRBEAST;02,ANDREW', '13': '02,MRBEAST;01,ANDREW', '14': '02,MRBEAST;03,ANDREW', '15': '02,MRBEAST;01,ANDREW', '16': '01,MRBEAST;00,ANDREW', '17': '02,MRBEAST;00,ANDREW', '18': '03,MRBEAST', '19': '02,MRBEAST;01,ANDREW', '20': '01,MRBEAST;02,ANDREW', '21': '00,MRBEAST;01,ANDREW', '22': '02,MRBEAST;03,ANDREW'}\n",
      "diarized_audio/raw_diarizations/lexFridman-georgeHotz-30062023\n",
      "{'1': '00,LEXFRIDMAN;01,GEORGEHOTZ', '2': '00,GEORGEHOTZ;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,LEXFRIDMAN;02,GEORGEHOTZ;03,GEORGEHOTZ;04,GEORGEHOTZ', '4': '00,LEXFRIDMAN;01,GEORGEHOTZ', '5': '00,LEXFRIDMAN;01,GEORGEHOTZ', '6': '00,GEORGEHOTZ;01,LEXFRIDMAN', '7': '00,GEORGEHOTZ;01,LEXFRIDMAN', '8': '00,LEXFRIDMAN;01,GEORGEHOTZ', '9': '00,GEORGEHOTZ;01,LEXFRIDMAN', '10': '00,LEXFRIDMAN;01,GEORGEHOTZ', '11': '00,LEXFRIDMAN;01,GEORGEHOTZ', '12': '00,GEORGEHOTZ;01,LEXFRIDMAN', '13': '00,LEXFRIDMAN;01,GEORGEHOTZ', '14': '00,LEXFRIDMAN;01,GEORGEHOTZ', '15': '00,GEORGEHOTZ;01,GEORGEHOTZ;02,LEXFRIDMAN', '16': '00,GEORGEHOTZ;01,LEXFRIDMAN', '17': '00,LEXFRIDMAN;01,GEORGEHOTZ;02,GEORGEHOTZ', '18': '00,GEORGEHOTZ;01,LEXFRIDMAN', '19': '00,LEXFRIDMAN;01,GEORGEHOTZ'}\n",
      "diarized_audio/raw_diarizations/flagrant-andrewHubermam-17102022\n",
      "{'1': '01,HUBERMAN', '2': '00,HUBERMAN', '3': '01,HUBERMAN', '4': '00,HUBERMAN', '5': '02,HUBERMAN;01,ANDREW', '6': '01,HUBERMAN;00,ANDREW', '7': '01,HUBERMAN', '8': '01,HUBERMAN;02,ANDREW', '9': '01,HUBERMAN;00,ANDREW', '10': '02,HUBERMAN;03,ANDREW', '11': '02,HUBERMAN;01,ANDREW', '12': '00,HUBERMAN', '13': '00,HUBERMAN;01,ANDREW'}\n",
      "diarized_audio/raw_diarizations/flagrant-MKBHD-01102022\n",
      "{'1': '01,MARQUES;03,ANDREW', '2': '00,MARQUES', '3': '01,MARQUES', '4': '01,MARQUES', '5': '04,MARQUES;02,ANDREW', '6': '00,MARQUES', '7': '00,MARQUES;01,ANDREW', '8': '02,MARQUES;03,ANDREW', '9': '01,MARQUES;03,ANDREW', '10': '00,MARQUES;02,ANDREW', '11': '02,MARQUES;03,ANDREW'}\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from speakers import replacer_dict\n",
    "\n",
    "get_speaker_turns(replacer_dict, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we get what a speaker has said in text, linking the diarizations with the transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to find the closest time in df2 to a given time in df1\n",
    "def find_closest_start_time(df, given_time):\n",
    "    # Calculate absolute time differences\n",
    "    \n",
    "    time_diff = (df['start'] - given_time).abs()\n",
    "    \n",
    "    # Find the index of the minimum difference\n",
    "    closest_index = time_diff.idxmin()\n",
    "    return closest_index\n",
    "\n",
    "def find_closest_end_time(df, given_time):\n",
    "    # Calculate absolute time differences\n",
    "    \n",
    "    time_diff = (df['end'] - given_time).abs()\n",
    "    \n",
    "    # Find the index of the minimum difference\n",
    "    closest_index = time_diff.idxmin()\n",
    "    return closest_index\n",
    "\n",
    "def add_text_to_diarization(diarization_df, transcriptions_df):\n",
    "\n",
    "    # New column for the combined text\n",
    "    diarization_df['text'] = ''\n",
    "\n",
    "    for index, row in diarization_df.iterrows():\n",
    "        # Find closest start and end times in df2\n",
    "        \n",
    "        closest_start_index = find_closest_start_time(transcriptions_df, row['start'])\n",
    "        closest_end_index = find_closest_end_time(transcriptions_df, row['end'])\n",
    "\n",
    "        # Extract all rows in-between these indices\n",
    "        if closest_start_index <= closest_end_index:\n",
    "            relevant_text = transcriptions_df.loc[closest_start_index:closest_end_index, 'text']\n",
    "        else:\n",
    "            relevant_text = transcriptions_df.loc[closest_end_index:closest_start_index, 'text']\n",
    "\n",
    "        # Combine the text and add to dataframe\n",
    "        combined_text = \"\".join(relevant_text)\n",
    "        diarization_df.at[index, 'text'] = combined_text\n",
    "\n",
    "    # dataframe now contains the combined text in the new 'text' column\n",
    "    return diarization_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all podcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get speaker-tagged transcriptions for each dataframe, and save them in the datasets directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarizations = \"diarized_audio/raw_diarizations\"\n",
    "\n",
    "directories = [\n",
    "    directory\n",
    "    for directory in os.listdir(diarizations) \n",
    "    if \"speaker_turns.csv\" in os.listdir(os.path.join(diarizations, directory))\n",
    "]\n",
    "\n",
    "for podcast_name in directories:\n",
    "    directory = os.path.join(diarizations, podcast_name)\n",
    "\n",
    "    diarization = pd.read_csv(f\"diarized_audio/raw_diarizations/{podcast_name}/speaker_turns.csv\")\n",
    "    transcription = pd.read_csv(f\"transcribed_audio/{podcast_name}_transcribed.csv\")\n",
    "\n",
    "    # Do a bit of data cleaning\n",
    "    transcription['start'] = pd.to_timedelta(transcription['start'], unit='s')\n",
    "    transcription['end'] = pd.to_timedelta(transcription['end'], unit='s')\n",
    "\n",
    "    diarization['start'] = pd.to_timedelta(diarization['start'])\n",
    "    diarization['end'] = pd.to_timedelta(diarization['end'])\n",
    "\n",
    "    all_transcriptions = \"\".join(transcription[\"text\"])\n",
    "\n",
    "    final_df = add_text_to_diarization(diarization, transcription)\n",
    "    final_df.to_csv(os.path.join(\"datasets\", podcast_name + \".csv\"), header=True, index=False)\n",
    "\n",
    "    # speaked = final_df.groupby(\"speaker\")['text'].apply(lambda x: ''.join(x))\n",
    "    # print(\"Number of words:\", len(tweet_tokenizer.tokenize(all_transcriptions)))\n",
    "    # print(\"Number of sentences:\", len(sent_tokenize(all_transcriptions)))\n",
    "    # print(speaked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then join them all in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"diarized_transcribed_df\"\n",
    "podcast_arr = []\n",
    "\n",
    "for podcast_name in directories:\n",
    "    podcast = pd.read_csv(os.path.join('datasets', podcast_name + '.csv'))\n",
    "    podcast_arr.append(podcast)\n",
    "\n",
    "diarized_transcribed_df = pd.concat(podcast_arr)\n",
    "diarized_transcribed_df.to_csv(os.path.join(\"datasets\", df_name + '.csv'), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = diarized_transcribed_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the podcasts to ensure everything is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['start'] = pd.to_timedelta(text_df['start'])\n",
    "text_df['end'] = pd.to_timedelta(text_df['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_df = (\n",
    "    text_df[\n",
    "        ~(( text_df['end'] - text_df['start'] ) < pd.Timedelta(1, unit='s'))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 228 duplicates\n"
     ]
    }
   ],
   "source": [
    "# Are there duplicates?\n",
    "print(f\"There are {fix_df['text'].duplicated().sum()} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "duplicates_across_speakers = fix_df[fix_df.duplicated(subset='text', keep=False)]\n",
    "\n",
    "unique_speakers_per_text = duplicates_across_speakers.drop_duplicates(subset=['text', 'speaker'])\n",
    "\n",
    "texts_to_remove = unique_speakers_per_text[unique_speakers_per_text.duplicated(subset='text', keep=False)]['text']\n",
    "\n",
    "# Remove entirely duplicate text if different speakers have said it\n",
    "fix_df = fix_df[~fix_df['text'].isin(texts_to_remove)]\n",
    "\n",
    "# Keep the first ocurrence of duplicate text if the same speaker has said it\n",
    "# (it could be a catch phrase, or something that identifies the speaker, which is very useful for speaker prediction)\n",
    "fix_df = fix_df.drop_duplicates(subset=['speaker', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that we have removed all the duplicates\n",
    "fix_df['text'].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_grouped_df = (\n",
    "    fix_df.groupby('speaker')['text']\n",
    "    .agg(\n",
    "        lambda x: ''.join(x)\n",
    "    ).to_frame().reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = text_df.groupby('speaker')['text'].agg(lambda x: ''.join(x)).to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['words'] = grouped_df['text'].apply(lambda x: len(tweet_tokenizer.tokenize(x)))\n",
    "grouped_df['sentences']  = grouped_df['text'].apply(lambda x: len(sent_tokenize(x)))\n",
    "\n",
    "fix_grouped_df['words'] = fix_grouped_df['text'].apply(lambda x: len(tweet_tokenizer.tokenize(x)))\n",
    "fix_grouped_df['sentences']  = fix_grouped_df['text'].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminated Words = 71115 out of 716957 (9.92%) | 716957 reduced to -> 645842 \n",
      "Eliminated Sentences = 5420 out of 52734 (10.28%) | 52734 reduced to -> 47314 \n"
     ]
    }
   ],
   "source": [
    "wordiff = sum(grouped_df['words'] - fix_grouped_df['words'])\n",
    "sentdiff = sum(grouped_df['sentences'] - fix_grouped_df['sentences'])\n",
    "\n",
    "print(f\"Eliminated Words = {wordiff} out of {grouped_df['words'].sum()} ({((wordiff / grouped_df['words'].sum()) * 100):.2f}%) | {grouped_df['words'].sum()} reduced to -> {fix_grouped_df['words'].sum()} \")\n",
    "print(f\"Eliminated Sentences = {sentdiff} out of {grouped_df['sentences'].sum()} ({((sentdiff / grouped_df['sentences'].sum()) * 100):.2f}%) | {grouped_df['sentences'].sum()} reduced to -> {fix_grouped_df['sentences'].sum()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANDREW</td>\n",
       "      <td>Flatter for archaeologists. Yeah, it's from t...</td>\n",
       "      <td>27446</td>\n",
       "      <td>3316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANDREWHUBERMAN</td>\n",
       "      <td>Listen, when it comes to romantic relationshi...</td>\n",
       "      <td>23377</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BENSHAPIRO</td>\n",
       "      <td>The great light we tell ourselves is that peo...</td>\n",
       "      <td>28319</td>\n",
       "      <td>1621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GEORGEHOTZ</td>\n",
       "      <td>Sure. So I think the most obvious way to me i...</td>\n",
       "      <td>23859</td>\n",
       "      <td>2055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GUIDOVANROSSUM</td>\n",
       "      <td>How did you pull that off? And what's C Pytho...</td>\n",
       "      <td>23889</td>\n",
       "      <td>1213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HANCOCK</td>\n",
       "      <td>We're almost at the edge of history when we g...</td>\n",
       "      <td>26843</td>\n",
       "      <td>2027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HUBERMAN</td>\n",
       "      <td>I need this answer. Big time, foot fetish guy...</td>\n",
       "      <td>26572</td>\n",
       "      <td>2200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>JOEROGAN</td>\n",
       "      <td>Joe. What's going on man? Yeah, you're really...</td>\n",
       "      <td>90216</td>\n",
       "      <td>7006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>JOEY</td>\n",
       "      <td>My brother. How you feeling? Like I'm a new f...</td>\n",
       "      <td>21456</td>\n",
       "      <td>2064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KANYE</td>\n",
       "      <td>Joe Rogan, why can't I check it out? Pichot R...</td>\n",
       "      <td>28416</td>\n",
       "      <td>1634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>KANYEWEST</td>\n",
       "      <td>Based off of our connection and just you bein...</td>\n",
       "      <td>21401</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>KEVINHART</td>\n",
       "      <td>You're always moving. I mean, is there anythi...</td>\n",
       "      <td>19435</td>\n",
       "      <td>1777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LEXFRIDMAN</td>\n",
       "      <td>Can you imagine possible features that Python...</td>\n",
       "      <td>62953</td>\n",
       "      <td>4611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MARKZUCKERBERG</td>\n",
       "      <td>that experience like? Oh, it's fun. I know. Y...</td>\n",
       "      <td>35694</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MARQUES</td>\n",
       "      <td>Yeah. This is new to me. Yeah. No, you emaile...</td>\n",
       "      <td>15168</td>\n",
       "      <td>1355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MATTHEWMCCOUNAGHEY</td>\n",
       "      <td>If you really want to give a character an obs...</td>\n",
       "      <td>17623</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>MIKETYSON</td>\n",
       "      <td>Hey, let's talk about that too. So, I was dis...</td>\n",
       "      <td>17857</td>\n",
       "      <td>1811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>MILEYCYRUS</td>\n",
       "      <td>I'm happy to be here. It makes you step back ...</td>\n",
       "      <td>22112</td>\n",
       "      <td>1854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>MRBEAST</td>\n",
       "      <td>I know all about YouTube analytics. Do you wa...</td>\n",
       "      <td>40322</td>\n",
       "      <td>4580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NILDEGRASSE</td>\n",
       "      <td>Good to see you. Thanks. Thanks. I feel a lit...</td>\n",
       "      <td>24686</td>\n",
       "      <td>2201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>POSTMALONE</td>\n",
       "      <td>Give you my ducks a brain That's my everybody...</td>\n",
       "      <td>19272</td>\n",
       "      <td>937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>SNOWDEN</td>\n",
       "      <td>uh... you how do you live in and things like ...</td>\n",
       "      <td>28926</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               speaker                                               text  \\\n",
       "0               ANDREW   Flatter for archaeologists. Yeah, it's from t...   \n",
       "1       ANDREWHUBERMAN   Listen, when it comes to romantic relationshi...   \n",
       "2           BENSHAPIRO   The great light we tell ourselves is that peo...   \n",
       "3           GEORGEHOTZ   Sure. So I think the most obvious way to me i...   \n",
       "4       GUIDOVANROSSUM   How did you pull that off? And what's C Pytho...   \n",
       "5              HANCOCK   We're almost at the edge of history when we g...   \n",
       "6             HUBERMAN   I need this answer. Big time, foot fetish guy...   \n",
       "7             JOEROGAN   Joe. What's going on man? Yeah, you're really...   \n",
       "8                 JOEY   My brother. How you feeling? Like I'm a new f...   \n",
       "9                KANYE   Joe Rogan, why can't I check it out? Pichot R...   \n",
       "10           KANYEWEST   Based off of our connection and just you bein...   \n",
       "11           KEVINHART   You're always moving. I mean, is there anythi...   \n",
       "12          LEXFRIDMAN   Can you imagine possible features that Python...   \n",
       "13      MARKZUCKERBERG   that experience like? Oh, it's fun. I know. Y...   \n",
       "14             MARQUES   Yeah. This is new to me. Yeah. No, you emaile...   \n",
       "15  MATTHEWMCCOUNAGHEY   If you really want to give a character an obs...   \n",
       "16           MIKETYSON   Hey, let's talk about that too. So, I was dis...   \n",
       "17          MILEYCYRUS   I'm happy to be here. It makes you step back ...   \n",
       "18             MRBEAST   I know all about YouTube analytics. Do you wa...   \n",
       "19         NILDEGRASSE   Good to see you. Thanks. Thanks. I feel a lit...   \n",
       "20          POSTMALONE   Give you my ducks a brain That's my everybody...   \n",
       "21             SNOWDEN   uh... you how do you live in and things like ...   \n",
       "\n",
       "    words  sentences  \n",
       "0   27446       3316  \n",
       "1   23377       1405  \n",
       "2   28319       1621  \n",
       "3   23859       2055  \n",
       "4   23889       1213  \n",
       "5   26843       2027  \n",
       "6   26572       2200  \n",
       "7   90216       7006  \n",
       "8   21456       2064  \n",
       "9   28416       1634  \n",
       "10  21401       1594  \n",
       "11  19435       1777  \n",
       "12  62953       4611  \n",
       "13  35694       1528  \n",
       "14  15168       1355  \n",
       "15  17623        524  \n",
       "16  17857       1811  \n",
       "17  22112       1854  \n",
       "18  40322       4580  \n",
       "19  24686       2201  \n",
       "20  19272        937  \n",
       "21  28926          1  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export this raw df\n",
    "fix_df.to_csv(os.path.join(\"datasets\", \"final_df_raw_interventions.csv\"), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export it just with raw sentences\n",
    "\n",
    "# Preprocess\n",
    "raw_sent_df = fix_grouped_df.copy(deep=True)\n",
    "raw_sent_df['sentences'] = raw_sent_df['text'].apply(sent_tokenize)\n",
    "\n",
    "raw_sent_df = raw_sent_df.explode('sentences').drop(['text', 'words'], axis=1).reset_index(drop=True)\n",
    "\n",
    "# Keep only sentences with more than three words, others are irrelevant\n",
    "raw_sent_df['word_count'] = raw_sent_df['sentences'].apply(lambda x: len(x.split()))\n",
    "raw_sent_df = raw_sent_df[raw_sent_df['word_count'] > 3].drop('word_count', axis=1)\n",
    "\n",
    "# Export it\n",
    "raw_sent_df.to_csv(os.path.join(\"datasets\", \"final_df_raw_sentences.csv\"), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter our df taking only speakers that have said more than 20k words\n",
    "speakers_of_interest = fix_grouped_df[fix_grouped_df['words'] > 20_000]['speaker']\n",
    "df = fix_df[fix_df['speaker'].isin(speakers_of_interest)][['speaker', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentences'] = df['text'].apply(sent_tokenize)\n",
    "df = df.explode('sentences').drop('text', axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only sentences with more than three words, others are irrelevant\n",
    "df['word_count'] = df['sentences'].apply(lambda x: len(x.split()))\n",
    "df = df[df['word_count'] > 3].drop('word_count', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_and_split_dataframe(df, class_column, split_prob=[0.7, 0.15, 0.15]):\n",
    "\n",
    "    #split parameters\n",
    "    choices = ['train', 'test', 'val']\n",
    "\n",
    "    # Group by class and find the smallest class size\n",
    "    group = df.groupby(class_column)\n",
    "    smallest_class_size = group.size().min()\n",
    "\n",
    "    # Sample from each class (we can use with multi-label)\n",
    "    undersampled_df = pd.DataFrame()\n",
    "    for _, group_df in group:\n",
    "        sampled_df = group_df.sample(n=smallest_class_size, replace=False, random_state=1)\n",
    "\n",
    "        # Perform the split class by class, for the train, test and validation rows to be balanced.\n",
    "        sampled_df['split'] = np.random.choice(choices, size=len(sampled_df), p=split_prob)\n",
    "        undersampled_df = pd.concat([undersampled_df, sampled_df], axis=0)\n",
    "\n",
    "    # Check the distribution\n",
    "    print(undersampled_df['split'].value_counts(normalize=True))\n",
    "\n",
    "    return undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "train    0.706885\n",
      "test     0.147393\n",
      "val      0.145722\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "prepared_df = undersample_and_split_dataframe(df, 'speaker')\n",
    "prepared_df.columns = ['category', 'title', 'split']\n",
    "prepared_df = prepared_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "ANDREW            352\n",
       "ANDREWHUBERMAN    352\n",
       "BENSHAPIRO        352\n",
       "GEORGEHOTZ        352\n",
       "GUIDOVANROSSUM    352\n",
       "HANCOCK           352\n",
       "HUBERMAN          352\n",
       "JOEROGAN          352\n",
       "JOEY              352\n",
       "KANYE             352\n",
       "KANYEWEST         352\n",
       "LEXFRIDMAN        352\n",
       "MARKZUCKERBERG    352\n",
       "MILEYCYRUS        352\n",
       "MRBEAST           352\n",
       "NILDEGRASSE       352\n",
       "SNOWDEN           352\n",
       "dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_df.groupby('category').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df.to_csv(os.path.join(\"datasets\", df_name + \"_preparado.csv\"), header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTANDWHISPER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
