{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path = \"diarized_audio/raw_diarizations\"\n",
    "diarization_paths = {}\n",
    "for directory in os.listdir(d_path):\n",
    "    diarization_paths[directory] = os.path.join(d_path, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, process all the files to have a suitable format. This will be a pandas dataframe, stored in memory as a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all csv files from diarization directory\n",
    "for path in os.listdir(d_path):\n",
    "    !rm -r {os.path.join(d_path, path)}/speaker_turns.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to_replace_dict = {\n",
    "#     \"1\":\"01,HUBERMAN;00,ANDREW\",\n",
    "#     \"2\":\"00,HUBERMAN;01,ANDREW\",\n",
    "#     \"3\":\"01,HUBERMAN;00,ANDREW\",\n",
    "#     \"4\":\"00,HUBERMAN;01,ANDREW\",\n",
    "#     \"5\":\"02,HUBERMAN;01,ANDREW\",\n",
    "#     \"6\":\"01,HUBERMAN;00,ANDREW\",\n",
    "#     \"7\":\"01,HUBERMAN\",\n",
    "#     \"8\":\"01,HUBERMAN;02,ANDREW\",\n",
    "#     \"9\":\"01,HUBERMAN;00,ANDREW\",\n",
    "#     \"10\":\"02,HUBERMAN;03,ANDREW\",\n",
    "#     \"11\":\"02,HUBERMAN;01,ANDREW\",\n",
    "#     \"12\":\"00,HUBERMAN\",\n",
    "#     \"13\":\"00,HUBERMAN;01,ANDREW\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_directories(arr):\n",
    "    def sort_key(s):\n",
    "        return int(s.split('_')[1])\n",
    "\n",
    "    return sorted(arr, key=sort_key)\n",
    "\n",
    "def parse_time(time_str):\n",
    "    # Parses the time string to a pandas.Timedelta object\n",
    "    return pd.Timedelta(time_str)\n",
    "\n",
    "def process_line(line):\n",
    "    # Splits the line and extracts the required information\n",
    "    time_data, speaker = line.split(']')\n",
    "    time_data = time_data[2:]\n",
    "    start_time, end_time = time_data.split(' -->  ')\n",
    "    speaker = speaker.split(' ')[-1]  # Gets only the SPEAKER_xx part\n",
    "    return speaker, parse_time(start_time), parse_time(end_time)\n",
    "\n",
    "def most_common_letters(string, dictionary):\n",
    "    def common_letter_count(s1, s2):\n",
    "        return sum(min(s1.count(c), s2.count(c)) for c in set(s1))\n",
    "\n",
    "    max_key = max(dictionary.keys(), key=lambda k: common_letter_count(k.lower(), string.lower()))\n",
    "    return dictionary[max_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_joerogan_kevinhart(to_replace_dict):\n",
    "    directory = \"diarized_audio/raw_diarizations/JRE-kevinHart-25052020\"\n",
    "\n",
    "    podcast_df = pd.DataFrame(columns=['speaker', 'start', 'end'])\n",
    "    spacer_time = pd.Timedelta('0 days 00:00:00.998000')\n",
    "    global_time = pd.Timedelta('0')\n",
    "    splits = sort_directories(os.listdir(directory))\n",
    "\n",
    "    for idx, split in enumerate(splits):\n",
    "        \n",
    "        file_path = os.path.join(directory, split)\n",
    "        # get split number with regex that searches for a number\n",
    "        number = re.search(r'\\d+(\\.\\d+)?', split).group()\n",
    "        # get the speakers (SPEAKER_00, SPEAKER_...) that talk in this split\n",
    "        #talkers = [\"SPEAKER_\" + n for n in re.split(\"[,;]\", to_replace_dict[number])[0::2]]\n",
    "        \n",
    "        if to_replace_dict[number] == \"\":   #TODO hay que utilizarlo pero no aqu√≠\n",
    "            continue\n",
    "\n",
    "        real_speakers = to_replace_dict[number].split(\";\")\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                speaker, start, end = process_line(line)\n",
    "\n",
    "                # Set the correct speaker\n",
    "                speaker = speaker.strip('\\n')\n",
    "\n",
    "                for rsp in real_speakers:\n",
    "                    rsp = rsp.split(\",\")\n",
    "                    pyannote_speaker = 'SPEAKER_' + rsp[0]\n",
    "                    \n",
    "                    if pyannote_speaker == speaker:\n",
    "                        speaker = rsp[1]\n",
    "                \n",
    "                # Adjust start and end to the global time\n",
    "                start = start + global_time - spacer_time\n",
    "                end = end + global_time - spacer_time\n",
    "                podcast_df.loc[len(podcast_df)] = [speaker, start, end]\n",
    "        \n",
    "        global_time = podcast_df[\"end\"].max()\n",
    "    \n",
    "    pattern = 'SPEAKER_\\d+'\n",
    "    filtered_df = podcast_df[~podcast_df['speaker'].str.contains(pattern, na=False)]\n",
    "    filtered_df.to_csv(os.path.join(directory, \"speaker_turns.csv\"), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_lexFridman(podcast_dictionaries):\n",
    "    diarizations = \"diarized_audio/raw_diarizations\"\n",
    "    directories = [\n",
    "        directory \n",
    "        for directory in os.listdir(diarizations) \n",
    "        if bool(re.match(r\"^lexFridman\", directory))\n",
    "    ]\n",
    "    \n",
    "    for podcast_name in directories:\n",
    "\n",
    "        directory = os.path.join(diarizations, podcast_name)\n",
    "        print(directory)\n",
    "        podcast_df = pd.DataFrame(columns=['speaker', 'start', 'end'])\n",
    "        spacer_time = pd.Timedelta('0 days 00:00:00.998000')\n",
    "        global_time = pd.Timedelta('0')\n",
    "        splits = sort_directories([d for d in os.listdir(directory) if d.endswith('.txt')])\n",
    "        #to_replace_dict = most_common_letters(podcast_name, podcast_dictionaries)\n",
    "        to_replace_dict = podcast_dictionaries[podcast_name]\n",
    "        print(to_replace_dict)\n",
    "        \n",
    "        for idx, split in enumerate(splits):\n",
    "            \n",
    "            file_path = os.path.join(directory, split)\n",
    "            # get split number with regex that searches for a number\n",
    "            number = re.search(r'\\d+(\\.\\d+)?', split).group()\n",
    "            # get the speakers (SPEAKER_00, SPEAKER_...) that talk in this split\n",
    "            #talkers = [\"SPEAKER_\" + n for n in re.split(\"[,;]\", to_replace_dict[number])[0::2]]\n",
    "\n",
    "            if to_replace_dict[number] == \"\":   #TODO modificar\n",
    "                continue\n",
    "\n",
    "            real_speakers = to_replace_dict[number].split(\";\")\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    speaker, start, end = process_line(line)\n",
    "\n",
    "                    # Set the correct speaker\n",
    "\n",
    "                    speaker = speaker.strip('\\n')\n",
    "                    for rsp in real_speakers:\n",
    "                        rsp = rsp.split(\",\")\n",
    "                        pyannote_speaker = 'SPEAKER_' + rsp[0]\n",
    "                        \n",
    "                        if pyannote_speaker == speaker:\n",
    "                            speaker = rsp[1]\n",
    "                    \n",
    "                    # Adjust start and end to the global time\n",
    "                    start = start + global_time - spacer_time\n",
    "                    end = end + global_time - spacer_time\n",
    "\n",
    "                    #Create a new tagged speaker intervention in the podcast\n",
    "                    podcast_df.loc[len(podcast_df)] = [speaker, start, end]\n",
    "            \n",
    "            global_time = podcast_df[\"end\"].max()\n",
    "    \n",
    "        pattern = 'SPEAKER_\\d+'\n",
    "        filtered_df = podcast_df[~podcast_df['speaker'].str.contains(pattern, na=False)]\n",
    "        filtered_df.to_csv(os.path.join(directory, \"speaker_turns.csv\"), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format will be split_number:speakers_with_replacement, where speakers\n",
    "# with replacement is a string with the number of speaker to replace and\n",
    "# the name, separated by a comma and separated by semicolon from other\n",
    "# speakers\n",
    "to_replace_dict = {\n",
    "    \"1\": \"00,KEVINHART;01,JOEROGAN\",\n",
    "    \"2\": \"00,KEVINHART;01,JOEROGAN\",\n",
    "    \"3\": \"00,KEVINHART;01,JOEROGAN\",\n",
    "    \"4\": \"00,JOEROGAN;01,KEVINHART\",\n",
    "    \"5\": \"00,JOEROGAN;01,KEVINHART\",\n",
    "    \"6\": \"00,KEVINHART;01,JOEROGAN\",\n",
    "    \"7\": \"00,KEVINHART;01,JOEROGAN\",\n",
    "    \"8\": \"00,KEVINHART;01,JOEROGAN\",\n",
    "    \"9\": \"01,KEVINHART;02,JOEROGAN\",\n",
    "    \"10\": \"00,KEVINHART;01,JOEROGAN\",\n",
    "    \"11\": \"00,KEVINHART;01,JOEROGAN\",\n",
    "    \"12\": \"00,KEVINHART;01,JOEROGAN\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacer_dict = {}\n",
    "\n",
    "to_replace_dict = {\n",
    "    \"1\": \"00,ANDREWHUBERMAN;01,LEXFRIDMAN\",\n",
    "    \"2\": \"00,ANDREWHUBERMAN;01,LEXFRIDMAN\",\n",
    "    \"3\": \"00,LEXFRIDMAN;01,ANDREWHUBERMAN\",\n",
    "    \"4\": \"00,LEXFRIDMAN;01,ANDREWHUBERMAN\",\n",
    "    \"5\": \"00,LEXFRIDMAN;01,ANDREWHUBERMAN\",\n",
    "    \"6\": \"00,LEXFRIDMAN;01,ANDREWHUBERMAN\",\n",
    "    \"7\": \"00,LEXFRIDMAN;01,ANDREWHUBERMAN\",\n",
    "    \"8\": \"00,LEXFRIDMAN;01,ANDREWHUBERMAN\",\n",
    "    \"9\": \"00,ANDREWHUBERMAN;01,LEXFRIDMAN\",\n",
    "    \"10\": \"00,LEXFRIDMAN;01,ANDREWHUBERMAN\",\n",
    "    \"11\": \"00,LEXFRIDMAN;01,ANDREWHUBERMAN\",\n",
    "    \"12\": \"00,LEXFRIDMAN;01,ANDREWHUBERMAN\",\n",
    "    \"13\": \"00,LEXFRIDMAN;01,ANDREWHUBERMAN\"\n",
    "}\n",
    "\n",
    "replacer_dict[\"lexFridman-andrewHuberman-17082023\"] = to_replace_dict\n",
    "\n",
    "to_replace_dict = {\n",
    "    \"1\": \"00,BENSHAPIRO;01,LEXFRIDMAN\",\n",
    "    \"2\": \"00,BENSHAPIRO;01,LEXFRIDMAN\",\n",
    "    \"3\": \"00,LEXFRIDMAN;01,BENSHAPIRO\",\n",
    "    \"4\": \"00,BENSHAPIRO;01,LEXFRIDMAN\",\n",
    "    \"5\": \"00,LEXFRIDMAN;01,BENSHAPIRO\",\n",
    "    \"6\": \"00,BENSHAPIRO;01,LEXFRIDMAN\",\n",
    "    \"7\": \"00,BENSHAPIRO;01,LEXFRIDMAN\",\n",
    "    \"8\": \"00,BENSHAPIRO;01,LEXFRIDMAN\",\n",
    "    \"9\": \"00,BENSHAPIRO;01,LEXFRIDMAN\",\n",
    "    \"10\": \"00,BENSHAPIRO;01,LEXFRIDMAN\",\n",
    "    \"11\": \"00,BENSHAPIRO;01,LEXFRIDMAN\",\n",
    "    \"12\": \"00,BENSHAPIRO;01,LEXFRIDMAN\",\n",
    "    \"13\": \"00,LEXFRIDMAN;01,BENSHAPIRO\",\n",
    "    \"14\": \"00,LEXFRIDMAN;01,BENSHAPIRO\",\n",
    "    \"15\": \"00,LEXFRIDMAN;01,BENSHAPIRO\"\n",
    "}\n",
    "\n",
    "replacer_dict[\"lexFridman-benShapiro-07112022\"] = to_replace_dict\n",
    "\n",
    "to_replace_dict = {\n",
    "    \"1\": \"00,GUIDOVANROSSUM;01,LEXFRIDMAN\",\n",
    "    \"2\": \"00,LEXFRIDMAN;01,GUIDOVANROSSUM\",\n",
    "    \"3\": \"00,LEXFRIDMAN;01,GUIDOVANROSSUM;02,LEXFRIDMAN\",\n",
    "    \"4\": \"00,LEXFRIDMAN;01,GUIDOVANROSSUM\",\n",
    "    \"5\": \"00,LEXFRIDMAN;01,GUIDOVANROSSUM\",\n",
    "    \"6\": \"00,LEXFRIDMAN;01,GUIDOVANROSSUM\",\n",
    "    \"7\": \"00,LEXFRIDMAN;01,GUIDOVANROSSUM\",\n",
    "    \"8\": \"00,GUIDOVANROSSUM;01,LEXFRIDMAN\",\n",
    "    \"9\": \"00,GUIDOVANROSSUM;01,LEXFRIDMAN\",\n",
    "    \"10\": \"00,LEXFRIDMAN;01,GUIDOVANROSSUM\",\n",
    "    \"11\": \"00,GUIDOVANROSSUM;01,LEXFRIDMAN\",\n",
    "    \"12\": \"00,LEXFRIDMAN;01,GUIDOVANROSSUM\",\n",
    "    \"13\": \"00,GUIDOVANROSSUM;01,LEXFRIDMAN\",\n",
    "    \"14\": \"00,GUIDOVANROSSUM;01,LEXFRIDMAN\",\n",
    "    \"15\": \"00,LEXFRIDMAN;01,GUIDOVANROSSUM\",\n",
    "    \"16\": \"00,LEXFRIDMAN;01,GUIDOVANROSSUM\",\n",
    "    \"17\": \"00,GUIDOVANROSSUM;01,LEXFRIDMAN\",\n",
    "    \"18\": \"00,LEXFRIDMAN;01,GUIDOVANROSSUM\",\n",
    "    \"19\": \"00,LEXFRIDMAN;01,GUIDOVANROSSUM\"\n",
    "}\n",
    "\n",
    "replacer_dict[\"lexFridman-guidoVanRossum-26112022\"] = to_replace_dict\n",
    "\n",
    "to_replace_dict = {\n",
    "    \"1\": \"00,LEXFRIDMAN;01,GEORGEHOTZ\",\n",
    "    \"2\": \"00,GEORGEHOTZ;01,LEXFRIDMAN\",\n",
    "    \"3\": \"00,LEXFRIDMAN;01,LEXFRIDMAN;02,GEORGEHOTZ;03,GEORGEHOTZ;04,GEORGEHOTZ\",\n",
    "    \"4\": \"00,LEXFRIDMAN;01,GEORGEHOTZ\",\n",
    "    \"5\": \"00,LEXFRIDMAN;01,GEORGEHOTZ\",\n",
    "    \"6\": \"00,GEORGEHOTZ;01,LEXFRIDMAN\",\n",
    "    \"7\": \"00,GEORGEHOTZ;01,LEXFRIDMAN\",\n",
    "    \"8\": \"00,LEXFRIDMAN;01,GEORGEHOTZ\",\n",
    "    \"9\": \"00,GEORGEHOTZ;01,LEXFRIDMAN\",\n",
    "    \"10\": \"00,LEXFRIDMAN;01,GEORGEHOTZ\",\n",
    "    \"11\": \"00,LEXFRIDMAN;01,GEORGEHOTZ\",\n",
    "    \"12\": \"00,GEORGEHOTZ;01,LEXFRIDMAN\",\n",
    "    \"13\": \"00,LEXFRIDMAN;01,GEORGEHOTZ\",\n",
    "    \"14\": \"00,LEXFRIDMAN;01,GEORGEHOTZ\",\n",
    "    \"15\": \"00,GEORGEHOTZ;01,GEORGEHOTZ;02,LEXFRIDMAN\",\n",
    "    \"16\": \"00,GEORGEHOTZ;01,LEXFRIDMAN\",\n",
    "    \"17\": \"00,LEXFRIDMAN;01,GEORGEHOTZ;02,GEORGEHOTZ\",\n",
    "    \"18\": \"00,GEORGEHOTZ;01,LEXFRIDMAN\",\n",
    "    \"19\": \"00,LEXFRIDMAN;01,GEORGEHOTZ\"\n",
    "}\n",
    "\n",
    "replacer_dict[\"lexFridman-georgeHotz-30062023\"] = to_replace_dict\n",
    "\n",
    "to_replace_dict = {\n",
    "    \"1\": \"00,LEXFRIDMAN;01,KANYEWEST\",\n",
    "    \"2\": \"00,KANYEWEST;01,LEXFRIDMAN\",\n",
    "    \"3\": \"00,LEXFRIDMAN;01,KANYEWEST\",\n",
    "    \"4\": \"00,LEXFRIDMAN;01,KANYEWEST\",\n",
    "    \"5\": \"00,KANYEWEST;01,LEXFRIDMAN\",\n",
    "    \"6\": \"00,LEXFRIDMAN;01,KANYEWEST\",\n",
    "    \"7\": \"00,KANYEWEST;01,LEXFRIDMAN\",\n",
    "    \"8\": \"00,KANYEWEST;01,LEXFRIDMAN\",\n",
    "    \"9\": \"00,KANYEWEST;01,LEXFRIDMAN\",\n",
    "    \"10\": \"00,LEXFRIDMAN;01,KANYEWEST\",\n",
    "    \"11\": \"00,LEXFRIDMAN;01,KANYEWEST\",\n",
    "    \"12\": \"00,LEXFRIDMAN;01,KANYEWEST\",\n",
    "    \"13\": \"00,KANYEWEST;01,LEXFRIDMAN\",\n",
    "    \"14\": \"00,LEXFRIDMAN;01,KANYEWEST\",\n",
    "    \"15\": \"00,KANYEWEST;01,LEXFRIDMAN\"\n",
    "}\n",
    "\n",
    "replacer_dict[\"lexFridman-kanyeWest-24102022\"] = to_replace_dict\n",
    "\n",
    "to_replace_dict = {\n",
    "    \"1\": \"00,LEXFRIDMAN;01,MARKZUCKERBERG\",\n",
    "    \"2\": \"00,MARKZUCKERBERG;01,LEXFRIDMAN\",\n",
    "    \"3\": \"00,LEXFRIDMAN;01,MARKZUCKERBERG\",\n",
    "    \"4\": \"00,MARKZUCKERBERG;01,LEXFRIDMAN\",\n",
    "    \"5\": \"00,MARKZUCKERBERG;01,LEXFRIDMAN\",\n",
    "    \"6\": \"00,LEXFRIDMAN;01,MARKZUCKERBERG\",\n",
    "    \"7\": \"00,LEXFRIDMAN;01,MARKZUCKERBERG\",\n",
    "    \"8\": \"00,LEXFRIDMAN;01,MARKZUCKERBERG\",\n",
    "    \"9\": \"00,LEXFRIDMAN;01,MARKZUCKERBERG\",\n",
    "    \"10\": \"00,LEXFRIDMAN;01,MARKZUCKERBERG\",\n",
    "    \"11\": \"00,MARKZUCKERBERG;01,LEXFRIDMAN\",\n",
    "    \"12\": \"00,MARKZUCKERBERG;01,LEXFRIDMAN\",\n",
    "    \"13\": \"00,MARKZUCKERBERG;01,LEXFRIDMAN\",\n",
    "    \"14\": \"00,LEXFRIDMAN;01,MARKZUCKERBERG\",\n",
    "    \"15\": \"00,LEXFRIDMAN;01,MARKZUCKERBERG\",\n",
    "    \"16\": \"00,LEXFRIDMAN;01,MARKZUCKERBERG\"\n",
    "}\n",
    "\n",
    "replacer_dict[\"lexFridman-markZuckerberg-09062023\"] = to_replace_dict\n",
    "\n",
    "to_replace_dict = {\n",
    "    \"1\": \"00,LEXFRIDMAN;01,MARKZUCKERBERG\",\n",
    "    \"2\": \"00,MARKZUCKERBERG;01,LEXFRIDMAN\",\n",
    "    \"3\": \"00,LEXFRIDMAN;01,MARKZUCKERBERG\",\n",
    "    \"4\": \"00,MARKZUCKERBERG;01,LEXFRIDMAN\",\n",
    "    \"5\": \"00,MARKZUCKERBERG;01,LEXFRIDMAN\",\n",
    "    \"6\": \"00,MARKZUCKERBERG;01,LEXFRIDMAN\"\n",
    "}\n",
    "\n",
    "replacer_dict[\"lexFridman-markZuckerberg-28092023\"] = to_replace_dict\n",
    "\n",
    "to_replace_dict = {\n",
    "    \"1\": \"00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN\",\n",
    "    \"2\": \"00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN\",\n",
    "    \"3\": \"00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN\",\n",
    "    \"4\": \"00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY\",\n",
    "    \"5\": \"00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN\",\n",
    "    \"6\": \"00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY\",\n",
    "    \"7\": \"00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY\",\n",
    "    \"8\": \"00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY\",\n",
    "    \"9\": \"00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY\",\n",
    "    \"10\": \"00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY\",\n",
    "    \"11\": \"00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY\",\n",
    "    \"12\": \"00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY\",\n",
    "    \"13\": \"00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY\",\n",
    "    \"14\": \"00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY\"\n",
    "}\n",
    "\n",
    "replacer_dict[\"lexFridman-matthewMcConaughey-13062023\"] = to_replace_dict\n",
    "\n",
    "to_replace_dict = {\n",
    "    \"1\": \"00,MRBEAST;01,LEXFRIDMAN\",\n",
    "    \"2\": \"00,MRBEAST;01,LEXFRIDMAN\",\n",
    "    \"3\": \"00,LEXFRIDMAN;01,MRBEAST\",\n",
    "    \"4\": \"00,MRBEAST;01,LEXFRIDMAN\",\n",
    "    \"5\": \"00,MRBEAST;01,LEXFRIDMAN\",\n",
    "    \"6\": \"00,MRBEAST;01,LEXFRIDMAN\",\n",
    "    \"7\": \"00,LEXFRIDMAN;01,MRBEAST\",\n",
    "    \"8\": \"00,MRBEAST;01,LEXFRIDMAN\",\n",
    "    \"9\": \"00,MRBEAST;01,LEXFRIDMAN\",\n",
    "    \"10\": \"00,MRBEAST;01,LEXFRIDMAN\",\n",
    "    \"11\": \"00,LEXFRIDMAN;01,MRBEAST\",\n",
    "    \"12\": \"00,LEXFRIDMAN;01,MRBEAST\",\n",
    "    \"13\": \"00,LEXFRIDMAN;01,MRBEAST\",\n",
    "    \"14\": \"00,LEXFRIDMAN;01,MRBEAST\"\n",
    "}\n",
    "\n",
    "replacer_dict[\"lexFridman-mrBeast-11012023\"] = to_replace_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diarized_audio/raw_diarizations/lexFridman-guidoVanRossum-26112022\n",
      "{'1': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '2': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '3': '00,LEXFRIDMAN;01,GUIDOVANROSSUM;02,LEXFRIDMAN', '4': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '5': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '6': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '7': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '8': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '9': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '10': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '11': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '12': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '13': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '14': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '15': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '16': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '17': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '18': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '19': '00,LEXFRIDMAN;01,GUIDOVANROSSUM'}\n",
      "diarized_audio/raw_diarizations/lexFridman-markZuckerberg-09062023\n",
      "{'1': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '2': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '4': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '5': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '6': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '7': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '8': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '9': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '10': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '11': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '12': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '13': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '14': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '15': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '16': '00,LEXFRIDMAN;01,MARKZUCKERBERG'}\n",
      "diarized_audio/raw_diarizations/lexFridman-matthewMcConaughey-13062023\n",
      "{'1': '00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN', '2': '00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN', '3': '00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN', '4': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '5': '00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN', '6': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '7': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '8': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '9': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '10': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '11': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '12': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '13': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '14': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY'}\n",
      "diarized_audio/raw_diarizations/lexFridman-andrewHuberman-17082023\n",
      "{'1': '00,ANDREWHUBERMAN;01,LEXFRIDMAN', '2': '00,ANDREWHUBERMAN;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '4': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '5': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '6': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '7': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '8': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '9': '00,ANDREWHUBERMAN;01,LEXFRIDMAN', '10': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '11': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '12': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '13': '00,LEXFRIDMAN;01,ANDREWHUBERMAN'}\n",
      "diarized_audio/raw_diarizations/lexFridman-kanyeWest-24102022\n",
      "{'1': '00,LEXFRIDMAN;01,KANYEWEST', '2': '00,KANYEWEST;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,KANYEWEST', '4': '00,LEXFRIDMAN;01,KANYEWEST', '5': '00,KANYEWEST;01,LEXFRIDMAN', '6': '00,LEXFRIDMAN;01,KANYEWEST', '7': '00,KANYEWEST;01,LEXFRIDMAN', '8': '00,KANYEWEST;01,LEXFRIDMAN', '9': '00,KANYEWEST;01,LEXFRIDMAN', '10': '00,LEXFRIDMAN;01,KANYEWEST', '11': '00,LEXFRIDMAN;01,KANYEWEST', '12': '00,LEXFRIDMAN;01,KANYEWEST', '13': '00,KANYEWEST;01,LEXFRIDMAN', '14': '00,LEXFRIDMAN;01,KANYEWEST', '15': '00,KANYEWEST;01,LEXFRIDMAN'}\n",
      "diarized_audio/raw_diarizations/lexFridman-markZuckerberg-28092023\n",
      "{'1': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '2': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '4': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '5': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '6': '00,MARKZUCKERBERG;01,LEXFRIDMAN'}\n",
      "diarized_audio/raw_diarizations/lexFridman-benShapiro-07112022\n",
      "{'1': '00,BENSHAPIRO;01,LEXFRIDMAN', '2': '00,BENSHAPIRO;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,BENSHAPIRO', '4': '00,BENSHAPIRO;01,LEXFRIDMAN', '5': '00,LEXFRIDMAN;01,BENSHAPIRO', '6': '00,BENSHAPIRO;01,LEXFRIDMAN', '7': '00,BENSHAPIRO;01,LEXFRIDMAN', '8': '00,BENSHAPIRO;01,LEXFRIDMAN', '9': '00,BENSHAPIRO;01,LEXFRIDMAN', '10': '00,BENSHAPIRO;01,LEXFRIDMAN', '11': '00,BENSHAPIRO;01,LEXFRIDMAN', '12': '00,BENSHAPIRO;01,LEXFRIDMAN', '13': '00,LEXFRIDMAN;01,BENSHAPIRO', '14': '00,LEXFRIDMAN;01,BENSHAPIRO', '15': '00,LEXFRIDMAN;01,BENSHAPIRO'}\n",
      "diarized_audio/raw_diarizations/lexFridman-mrBeast-11012023\n",
      "{'1': '00,MRBEAST;01,LEXFRIDMAN', '2': '00,MRBEAST;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,MRBEAST', '4': '00,MRBEAST;01,LEXFRIDMAN', '5': '00,MRBEAST;01,LEXFRIDMAN', '6': '00,MRBEAST;01,LEXFRIDMAN', '7': '00,LEXFRIDMAN;01,MRBEAST', '8': '00,MRBEAST;01,LEXFRIDMAN', '9': '00,MRBEAST;01,LEXFRIDMAN', '10': '00,MRBEAST;01,LEXFRIDMAN', '11': '00,LEXFRIDMAN;01,MRBEAST', '12': '00,LEXFRIDMAN;01,MRBEAST', '13': '00,LEXFRIDMAN;01,MRBEAST', '14': '00,LEXFRIDMAN;01,MRBEAST'}\n",
      "diarized_audio/raw_diarizations/lexFridman-georgeHotz-30062023\n",
      "{'1': '00,LEXFRIDMAN;01,GEORGEHOTZ', '2': '00,GEORGEHOTZ;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,LEXFRIDMAN;02,GEORGEHOTZ;03,GEORGEHOTZ;04,GEORGEHOTZ', '4': '00,LEXFRIDMAN;01,GEORGEHOTZ', '5': '00,LEXFRIDMAN;01,GEORGEHOTZ', '6': '00,GEORGEHOTZ;01,LEXFRIDMAN', '7': '00,GEORGEHOTZ;01,LEXFRIDMAN', '8': '00,LEXFRIDMAN;01,GEORGEHOTZ', '9': '00,GEORGEHOTZ;01,LEXFRIDMAN', '10': '00,LEXFRIDMAN;01,GEORGEHOTZ', '11': '00,LEXFRIDMAN;01,GEORGEHOTZ', '12': '00,GEORGEHOTZ;01,LEXFRIDMAN', '13': '00,LEXFRIDMAN;01,GEORGEHOTZ', '14': '00,LEXFRIDMAN;01,GEORGEHOTZ', '15': '00,GEORGEHOTZ;01,GEORGEHOTZ;02,LEXFRIDMAN', '16': '00,GEORGEHOTZ;01,LEXFRIDMAN', '17': '00,LEXFRIDMAN;01,GEORGEHOTZ;02,GEORGEHOTZ', '18': '00,GEORGEHOTZ;01,LEXFRIDMAN', '19': '00,LEXFRIDMAN;01,GEORGEHOTZ'}\n"
     ]
    }
   ],
   "source": [
    "#process_joerogan_kevinhart(to_replace_dict)\n",
    "process_lexFridman(replacer_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we get what a speaker has said in text, linking the diarizations with the transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to find the closest time in df2 to a given time in df1\n",
    "def find_closest_start_time(df, given_time):\n",
    "    # Calculate absolute time differences\n",
    "    \n",
    "    time_diff = (df['start'] - given_time).abs()\n",
    "    \n",
    "    # Find the index of the minimum difference\n",
    "    closest_index = time_diff.idxmin()\n",
    "    return closest_index\n",
    "\n",
    "def find_closest_end_time(df, given_time):\n",
    "    # Calculate absolute time differences\n",
    "    \n",
    "    time_diff = (df['end'] - given_time).abs()\n",
    "    \n",
    "    # Find the index of the minimum difference\n",
    "    closest_index = time_diff.idxmin()\n",
    "    return closest_index\n",
    "\n",
    "def add_text_to_diarization(diarization_df, transcriptions_df, output_name):\n",
    "\n",
    "    # New column for the combined text\n",
    "    diarization_df['text'] = ''\n",
    "\n",
    "    for index, row in diarization_df.iterrows():\n",
    "        # Find closest start and end times in df2\n",
    "        \n",
    "        closest_start_index = find_closest_start_time(transcriptions_df, row['start'])\n",
    "        closest_end_index = find_closest_end_time(transcriptions_df, row['end'])\n",
    "\n",
    "        # Extract all rows in-between these indices\n",
    "        if closest_start_index <= closest_end_index:\n",
    "            relevant_text = transcriptions_df.loc[closest_start_index:closest_end_index, 'text']\n",
    "        else:\n",
    "            relevant_text = transcriptions_df.loc[closest_end_index:closest_start_index, 'text']\n",
    "\n",
    "        # Combine the text and add to dataframe\n",
    "        combined_text = \"\".join(relevant_text)\n",
    "        diarization_df.at[index, 'text'] = combined_text\n",
    "\n",
    "    # dataframe now contains the combined text in the new 'text' column\n",
    "    diarization_df.to_csv(os.path.join(\"datasets\", output_name + \".csv\"), header=True, index=False)\n",
    "\n",
    "    return diarization_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joe Rogan & Kevin Hart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization = pd.read_csv(\"diarized_audio/raw_diarizations/JRE-kevinHart-25052020/speaker_turns.csv\")\n",
    "transcription = pd.read_csv(\"transcribed_audio/JRE-kevinHart-25052020_transcribed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription['start'] = pd.to_timedelta(transcription['start'], unit='s')\n",
    "transcription['end'] = pd.to_timedelta(transcription['end'], unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarization['start'] = pd.to_timedelta(diarization['start'])\n",
    "diarization['end'] = pd.to_timedelta(diarization['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 25477\n",
      "Number of sentences: 2325\n"
     ]
    }
   ],
   "source": [
    "all_transcriptions = \"\".join(transcription[\"text\"])\n",
    "print(\"Number of words:\", len(tweet_tokenizer.tokenize(all_transcriptions)))\n",
    "print(\"Number of sentences:\", len(sent_tokenize(all_transcriptions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = add_text_to_diarization(diarization, transcription, 'JRE_kevinHart_25052020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speaker\n",
       "JOEROGAN      You come in here moving in shaking, man. You ...\n",
       "KEVINHART     You're always moving. I mean, is there anythi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaked = final_df.groupby(\"speaker\")['text'].apply(lambda x: ''.join(x))\n",
    "speaked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All Lex Fridman Podcasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 30192\n",
      "Number of sentences: 1642\n",
      "speaker\n",
      "GUIDOVANROSSUM     of the new 4.0? Given the amount of pain and ...\n",
      "LEXFRIDMAN         Can you imagine possible features that Python...\n",
      "Name: text, dtype: object\n",
      "Number of words: 31351\n",
      "Number of sentences: 1443\n",
      "speaker\n",
      "LEXFRIDMAN         The following is a conversation with Mark Zuc...\n",
      "MARKZUCKERBERG     that experience like? Oh, it's fun. I know. Y...\n",
      "Name: text, dtype: object\n",
      "Number of words: 21661\n",
      "Number of sentences: 643\n",
      "speaker\n",
      "LEXFRIDMAN             The following is a conversation with Matthew ...\n",
      "MATTHEWMCCOUNAGHEY     If you really want to give a character an obs...\n",
      "Name: text, dtype: object\n",
      "Number of words: 27534\n",
      "Number of sentences: 1724\n",
      "speaker\n",
      "ANDREWHUBERMAN     Listen, when it comes to romantic relationshi...\n",
      "LEXFRIDMAN         The following is a conversation with my dear ...\n",
      "Name: text, dtype: object\n",
      "Number of words: 26227\n",
      "Number of sentences: 2085\n",
      "speaker\n",
      "KANYEWEST      Based off of our connection and just you bein...\n",
      "LEXFRIDMAN     The following is a conversation with Yay, the...\n",
      "Name: text, dtype: object\n",
      "Number of words: 13552\n",
      "Number of sentences: 632\n",
      "speaker\n",
      "LEXFRIDMAN         The following is a conversation with Mark Zuc...\n",
      "MARKZUCKERBERG     Yeah, we can put the light anywhere. No, it d...\n",
      "Name: text, dtype: object\n",
      "Number of words: 34944\n",
      "Number of sentences: 2029\n",
      "speaker\n",
      "BENSHAPIRO     The great light we tell ourselves is that peo...\n",
      "LEXFRIDMAN     and the possibility that it takes you over. D...\n",
      "Name: text, dtype: object\n",
      "Number of words: 35459\n",
      "Number of sentences: 2419\n",
      "speaker\n",
      "LEXFRIDMAN     I'm here with Mr. Beast, the brilliant master...\n",
      "MRBEAST        I think maybe one of the videos we've already...\n",
      "Name: text, dtype: object\n",
      "Number of words: 40603\n",
      "Number of sentences: 3665\n",
      "speaker\n",
      "GEORGEHOTZ     Sure. So I think the most obvious way to me i...\n",
      "LEXFRIDMAN     What possible ideas do you have for the whole...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Get lex fridman podcasts\n",
    "diarizations = \"diarized_audio/raw_diarizations\"\n",
    "directories = [\n",
    "    directory \n",
    "    for directory in os.listdir(diarizations) \n",
    "    if bool(re.match(r\"^lexFridman\", directory))\n",
    "]\n",
    "\n",
    "for podcast_name in directories:\n",
    "    directory = os.path.join(diarizations, podcast_name)\n",
    "\n",
    "    diarization = pd.read_csv(f\"diarized_audio/raw_diarizations/{podcast_name}/speaker_turns.csv\")\n",
    "    transcription = pd.read_csv(f\"transcribed_audio/{podcast_name}_transcribed.csv\")\n",
    "\n",
    "    # Do a bit of data cleaning\n",
    "    transcription['start'] = pd.to_timedelta(transcription['start'], unit='s')\n",
    "    transcription['end'] = pd.to_timedelta(transcription['end'], unit='s')\n",
    "\n",
    "    diarization['start'] = pd.to_timedelta(diarization['start'])\n",
    "    diarization['end'] = pd.to_timedelta(diarization['end'])\n",
    "\n",
    "    all_transcriptions = \"\".join(transcription[\"text\"])\n",
    "\n",
    "    final_df = add_text_to_diarization(diarization, transcription, podcast_name)\n",
    "    speaked = final_df.groupby(\"speaker\")['text'].apply(lambda x: ''.join(x))\n",
    "    print(\"Number of words:\", len(tweet_tokenizer.tokenize(all_transcriptions)))\n",
    "    print(\"Number of sentences:\", len(sent_tokenize(all_transcriptions)))\n",
    "    print(speaked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create lexFridman - others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexfridman_podcasts = []\n",
    "for podcast_name in directories:\n",
    "    podcast = pd.read_csv(f\"datasets/{podcast_name}.csv\")\n",
    "    #print(podcast.shape[0])\n",
    "    lexfridman_podcasts.append(podcast)\n",
    "\n",
    "lexfridman_podcasts_df = pd.concat(lexfridman_podcasts)\n",
    "lexfridman_podcasts_df['speaker'] = lexfridman_podcasts_df['speaker'].map(lambda x: 'RESTO' if x != 'LEXFRIDMAN' else 'LEXFRIDMAN')\n",
    "lexfridman_podcasts_df.to_csv(os.path.join(\"datasets\", \"all_lexFridman_resto.csv\"), header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTANDWHISPER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
