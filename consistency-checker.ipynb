{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_path = \"diarized_audio/raw_diarizations\"\n",
    "diarization_paths = {}\n",
    "for directory in os.listdir(d_path):\n",
    "    diarization_paths[directory] = os.path.join(d_path, directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Careful!` -- the following cell deletes previous work. If intended, run to make way for the new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove 'diarized_audio/raw_diarizations/JRE-neildeGrasseTyson-06092019/speaker_turns.csv': No such file or directory\n",
      "rm: cannot remove 'diarized_audio/raw_diarizations/JRE-edwardSnowden-23102019/speaker_turns.csv': No such file or directory\n",
      "rm: cannot remove 'diarized_audio/raw_diarizations/JRE-mikeTyson-04092020/speaker_turns.csv': No such file or directory\n",
      "rm: cannot remove 'diarized_audio/raw_diarizations/JRE-postMalone-29072020/speaker_turns.csv': No such file or directory\n",
      "rm: cannot remove 'diarized_audio/raw_diarizations/JRE-kanyeWest-24102020/speaker_turns.csv': No such file or directory\n",
      "rm: cannot remove 'diarized_audio/raw_diarizations/JRE-kevinHart-25052020/speaker_turns.csv': No such file or directory\n",
      "rm: cannot remove 'diarized_audio/raw_diarizations/JRE-joeyDiaz-26032020/speaker_turns.csv': No such file or directory\n",
      "rm: cannot remove 'diarized_audio/raw_diarizations/JRE-mileyCirus-02092020/speaker_turns.csv': No such file or directory\n",
      "rm: cannot remove 'diarized_audio/raw_diarizations/flagrant-joeRogan-07062022/speaker_turns.csv': No such file or directory\n",
      "rm: cannot remove 'diarized_audio/raw_diarizations/JRE-wizKhalifa-31032019/speaker_turns.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# # Remove all csv files from diarization directory\n",
    "for path in os.listdir(d_path):\n",
    "    !rm -r {os.path.join(d_path, path)}/speaker_turns.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_directories(arr):\n",
    "    def sort_key(s):\n",
    "        return int(s.split('_')[1])\n",
    "\n",
    "    return sorted(arr, key=sort_key)\n",
    "\n",
    "def parse_time(time_str):\n",
    "    # Parses the time string to a pandas.Timedelta object\n",
    "    return pd.Timedelta(time_str)\n",
    "\n",
    "def process_line(line):\n",
    "    # Splits the line and extracts the required information\n",
    "    time_data, speaker = line.split(']')\n",
    "    time_data = time_data[2:]\n",
    "    start_time, end_time = time_data.split(' -->  ')\n",
    "    speaker = speaker.split(' ')[-1]  # Gets only the SPEAKER_xx part\n",
    "    return speaker, parse_time(start_time), parse_time(end_time)\n",
    "\n",
    "def most_common_letters(string, dictionary):\n",
    "    def common_letter_count(s1, s2):\n",
    "        return sum(min(s1.count(c), s2.count(c)) for c in set(s1))\n",
    "\n",
    "    max_key = max(dictionary.keys(), key=lambda k: common_letter_count(k.lower(), string.lower()))\n",
    "    return dictionary[max_key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_speaker_turns(podcast_dictionaries, verbose : bool = False):\n",
    "    diarizations = \"diarized_audio/raw_diarizations\"\n",
    "    directories = [\n",
    "        directory \n",
    "        for directory in os.listdir(diarizations) \n",
    "        if directory in podcast_dictionaries.keys()\n",
    "    ]\n",
    "    \n",
    "    for podcast_name in directories:\n",
    "\n",
    "        directory = os.path.join(diarizations, podcast_name)\n",
    "        podcast_df = pd.DataFrame(columns=['speaker', 'start', 'end'])\n",
    "        spacer_time = pd.Timedelta('0 days 00:00:00.998000')\n",
    "        global_time = pd.Timedelta('0')\n",
    "        splits = sort_directories([d for d in os.listdir(directory) if d.endswith('.txt')])\n",
    "        #to_replace_dict = most_common_letters(podcast_name, podcast_dictionaries)\n",
    "        to_replace_dict = podcast_dictionaries[podcast_name]\n",
    "        \n",
    "        if verbose: \n",
    "            print(directory)\n",
    "            print(to_replace_dict)\n",
    "        \n",
    "        for idx, split in enumerate(splits):\n",
    "            \n",
    "            file_path = os.path.join(directory, split)\n",
    "            # get split number with regex that searches for a number\n",
    "            number = re.search(r'\\d+(\\.\\d+)?', split).group()\n",
    "            # get the speakers (SPEAKER_00, SPEAKER_...) that talk in this split\n",
    "\n",
    "            # In case we want to discard the data in one of the splits, just go to the next split.\n",
    "            if to_replace_dict[number] == \"\":\n",
    "                continue\n",
    "\n",
    "            real_speakers = to_replace_dict[number].split(\";\")\n",
    "\n",
    "            with open(file_path, 'r') as file:\n",
    "                for line in file:\n",
    "                    speaker, start, end = process_line(line)\n",
    "\n",
    "                    # Set the correct speaker\n",
    "\n",
    "                    speaker = speaker.strip('\\n')\n",
    "                    for rsp in real_speakers:\n",
    "                        rsp = rsp.split(\",\")\n",
    "                        pyannote_speaker = 'SPEAKER_' + rsp[0]\n",
    "                        \n",
    "                        if pyannote_speaker == speaker:\n",
    "                            speaker = rsp[1]\n",
    "                    \n",
    "                    # Adjust start and end to the global time\n",
    "                    start = start + global_time - spacer_time\n",
    "                    end = end + global_time - spacer_time\n",
    "\n",
    "                    #Create a new tagged speaker intervention in the podcast\n",
    "                    podcast_df.loc[len(podcast_df)] = [speaker, start, end]\n",
    "            \n",
    "            global_time = podcast_df[\"end\"].max()\n",
    "    \n",
    "        pattern = 'SPEAKER_\\d+'\n",
    "        filtered_df = podcast_df[~podcast_df['speaker'].str.contains(pattern, na=False)]\n",
    "        filtered_df.to_csv(os.path.join(directory, \"speaker_turns.csv\"), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diarized_audio/raw_diarizations/lexFridman-guidoVanRossum-26112022\n",
      "{'1': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '2': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '3': '00,LEXFRIDMAN;01,GUIDOVANROSSUM;02,LEXFRIDMAN', '4': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '5': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '6': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '7': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '8': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '9': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '10': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '11': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '12': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '13': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '14': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '15': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '16': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '17': '00,GUIDOVANROSSUM;01,LEXFRIDMAN', '18': '00,LEXFRIDMAN;01,GUIDOVANROSSUM', '19': '00,LEXFRIDMAN;01,GUIDOVANROSSUM'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diarized_audio/raw_diarizations/lexFridman-markZuckerberg-09062023\n",
      "{'1': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '2': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '4': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '5': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '6': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '7': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '8': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '9': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '10': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '11': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '12': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '13': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '14': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '15': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '16': '00,LEXFRIDMAN;01,MARKZUCKERBERG'}\n",
      "diarized_audio/raw_diarizations/lexFridman-matthewMcConaughey-13062023\n",
      "{'1': '00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN', '2': '00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN', '3': '00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN', '4': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '5': '00,MATTHEWMCCOUNAGHEY;01,LEXFRIDMAN', '6': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '7': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '8': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '9': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '10': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '11': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '12': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '13': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY', '14': '00,LEXFRIDMAN;01,MATTHEWMCCOUNAGHEY'}\n",
      "diarized_audio/raw_diarizations/lexFridman-andrewHuberman-17082023\n",
      "{'1': '00,ANDREWHUBERMAN;01,LEXFRIDMAN', '2': '00,ANDREWHUBERMAN;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '4': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '5': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '6': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '7': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '8': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '9': '00,ANDREWHUBERMAN;01,LEXFRIDMAN', '10': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '11': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '12': '00,LEXFRIDMAN;01,ANDREWHUBERMAN', '13': '00,LEXFRIDMAN;01,ANDREWHUBERMAN'}\n",
      "diarized_audio/raw_diarizations/flagrant-grahamHancock-27062023\n",
      "{'1': '01,HANCOCK;00,ANDREW', '2': '01,HANCOCK;00,ANDREW', '3': '03,HANCOCK;02,ANDREW', '4': '01,HANCOCK;02,ANDREW', '5': '01,HANCOCK;02,ANDREW', '6': '00,HANCOCK', '7': '01,HANCOCK', '8': '02,HANCOCK;01,ANDREW', '9': '01,HANCOCK;02,ANDREW', '10': '00,HANCOCK;01,ANDREW', '11': '00,HANCOCK;01,ANDREW', '12': '00,HANCOCK;01,ANDREW', '13': '02,HANCOCK', '14': '02,HANCOCK;01,ANDREW', '15': '03,HANCOCK;00,ANDREW', '16': '', '17': '00,HANCOCK;02,ANDREW', '18': '00,HANCOCK'}\n",
      "diarized_audio/raw_diarizations/lexFridman-kanyeWest-24102022\n",
      "{'1': '00,LEXFRIDMAN;01,KANYEWEST', '2': '00,KANYEWEST;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,KANYEWEST', '4': '00,LEXFRIDMAN;01,KANYEWEST', '5': '00,KANYEWEST;01,LEXFRIDMAN', '6': '00,LEXFRIDMAN;01,KANYEWEST', '7': '00,KANYEWEST;01,LEXFRIDMAN', '8': '00,KANYEWEST;01,LEXFRIDMAN', '9': '00,KANYEWEST;01,LEXFRIDMAN', '10': '00,LEXFRIDMAN;01,KANYEWEST', '11': '00,LEXFRIDMAN;01,KANYEWEST', '12': '00,LEXFRIDMAN;01,KANYEWEST', '13': '00,KANYEWEST;01,LEXFRIDMAN', '14': '00,LEXFRIDMAN;01,KANYEWEST', '15': '00,KANYEWEST;01,LEXFRIDMAN'}\n",
      "diarized_audio/raw_diarizations/lexFridman-markZuckerberg-28092023\n",
      "{'1': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '2': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,MARKZUCKERBERG', '4': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '5': '00,MARKZUCKERBERG;01,LEXFRIDMAN', '6': '00,MARKZUCKERBERG;01,LEXFRIDMAN'}\n",
      "diarized_audio/raw_diarizations/lexFridman-benShapiro-07112022\n",
      "{'1': '00,BENSHAPIRO;01,LEXFRIDMAN', '2': '00,BENSHAPIRO;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,BENSHAPIRO', '4': '00,BENSHAPIRO;01,LEXFRIDMAN', '5': '00,LEXFRIDMAN;01,BENSHAPIRO', '6': '00,BENSHAPIRO;01,LEXFRIDMAN', '7': '00,BENSHAPIRO;01,LEXFRIDMAN', '8': '00,BENSHAPIRO;01,LEXFRIDMAN', '9': '00,BENSHAPIRO;01,LEXFRIDMAN', '10': '00,BENSHAPIRO;01,LEXFRIDMAN', '11': '00,BENSHAPIRO;01,LEXFRIDMAN', '12': '00,BENSHAPIRO;01,LEXFRIDMAN', '13': '00,LEXFRIDMAN;01,BENSHAPIRO', '14': '00,LEXFRIDMAN;01,BENSHAPIRO', '15': '00,LEXFRIDMAN;01,BENSHAPIRO'}\n",
      "diarized_audio/raw_diarizations/lexFridman-mrBeast-11012023\n",
      "{'1': '00,MRBEAST;01,LEXFRIDMAN', '2': '00,MRBEAST;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,MRBEAST', '4': '00,MRBEAST;01,LEXFRIDMAN', '5': '00,MRBEAST;01,LEXFRIDMAN', '6': '00,MRBEAST;01,LEXFRIDMAN', '7': '00,LEXFRIDMAN;01,MRBEAST', '8': '00,MRBEAST;01,LEXFRIDMAN', '9': '00,MRBEAST;01,LEXFRIDMAN', '10': '00,MRBEAST;01,LEXFRIDMAN', '11': '00,LEXFRIDMAN;01,MRBEAST', '12': '00,LEXFRIDMAN;01,MRBEAST', '13': '00,LEXFRIDMAN;01,MRBEAST', '14': '00,LEXFRIDMAN;01,MRBEAST'}\n",
      "diarized_audio/raw_diarizations/flagrant-mrBeast-27092022\n",
      "{'1': '00,MRBEAST', '2': '00,MRBEAST;03,ANDREW', '3': '01,MRBEAST', '4': '00,MRBEAST', '5': '01,MRBEAST;00,ANDREW', '6': '01,MRBEAST;00,ANDREW', '7': '00,MRBEAST;01,ANDREW', '8': '03,MRBEAST;02,ANDREW', '9': '00,MRBEAST;01,ANDREW', '10': '02,MRBEAST', '11': '02,MRBEAST;01,ANDREW', '12': '00,MRBEAST;02,ANDREW', '13': '02,MRBEAST;01,ANDREW', '14': '02,MRBEAST;03,ANDREW', '15': '02,MRBEAST;01,ANDREW', '16': '01,MRBEAST;00,ANDREW', '17': '02,MRBEAST;00,ANDREW', '18': '03,MRBEAST', '19': '02,MRBEAST;01,ANDREW', '20': '01,MRBEAST;02,ANDREW', '21': '00,MRBEAST;01,ANDREW', '22': '02,MRBEAST;03,ANDREW'}\n",
      "diarized_audio/raw_diarizations/lexFridman-georgeHotz-30062023\n",
      "{'1': '00,LEXFRIDMAN;01,GEORGEHOTZ', '2': '00,GEORGEHOTZ;01,LEXFRIDMAN', '3': '00,LEXFRIDMAN;01,LEXFRIDMAN;02,GEORGEHOTZ;03,GEORGEHOTZ;04,GEORGEHOTZ', '4': '00,LEXFRIDMAN;01,GEORGEHOTZ', '5': '00,LEXFRIDMAN;01,GEORGEHOTZ', '6': '00,GEORGEHOTZ;01,LEXFRIDMAN', '7': '00,GEORGEHOTZ;01,LEXFRIDMAN', '8': '00,LEXFRIDMAN;01,GEORGEHOTZ', '9': '00,GEORGEHOTZ;01,LEXFRIDMAN', '10': '00,LEXFRIDMAN;01,GEORGEHOTZ', '11': '00,LEXFRIDMAN;01,GEORGEHOTZ', '12': '00,GEORGEHOTZ;01,LEXFRIDMAN', '13': '00,LEXFRIDMAN;01,GEORGEHOTZ', '14': '00,LEXFRIDMAN;01,GEORGEHOTZ', '15': '00,GEORGEHOTZ;01,GEORGEHOTZ;02,LEXFRIDMAN', '16': '00,GEORGEHOTZ;01,LEXFRIDMAN', '17': '00,LEXFRIDMAN;01,GEORGEHOTZ;02,GEORGEHOTZ', '18': '00,GEORGEHOTZ;01,LEXFRIDMAN', '19': '00,LEXFRIDMAN;01,GEORGEHOTZ'}\n",
      "diarized_audio/raw_diarizations/flagrant-andrewHubermam-17102022\n",
      "{'1': '01,ANDREWHUBERMAN', '2': '00,ANDREWHUBERMAN', '3': '01,ANDREWHUBERMAN', '4': '00,ANDREWHUBERMAN', '5': '02,ANDREWHUBERMAN;01,ANDREW', '6': '01,ANDREWHUBERMAN;00,ANDREW', '7': '01,ANDREWHUBERMAN', '8': '01,ANDREWHUBERMAN;02,ANDREW', '9': '01,ANDREWHUBERMAN;00,ANDREW', '10': '02,ANDREWHUBERMAN;03,ANDREW', '11': '02,ANDREWHUBERMAN;01,ANDREW', '12': '00,ANDREWHUBERMAN', '13': '00,ANDREWHUBERMAN;01,ANDREW'}\n",
      "diarized_audio/raw_diarizations/flagrant-MKBHD-01102022\n",
      "{'1': '01,MARQUES;03,ANDREW', '2': '00,MARQUES', '3': '01,MARQUES', '4': '01,MARQUES', '5': '04,MARQUES;02,ANDREW', '6': '00,MARQUES', '7': '00,MARQUES;01,ANDREW', '8': '02,MARQUES;03,ANDREW', '9': '01,MARQUES;03,ANDREW', '10': '00,MARQUES;02,ANDREW', '11': '02,MARQUES;03,ANDREW'}\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from speakers import replacer_dict\n",
    "\n",
    "get_speaker_turns(replacer_dict, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we get what a speaker has said in text, linking the diarizations with the transcriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to find the closest time in df2 to a given time in df1\n",
    "def find_closest_start_time(df, given_time):\n",
    "    # Calculate absolute time differences\n",
    "    \n",
    "    time_diff = (df['start'] - given_time).abs()\n",
    "    \n",
    "    # Find the index of the minimum difference\n",
    "    closest_index = time_diff.idxmin()\n",
    "    return closest_index\n",
    "\n",
    "def find_closest_end_time(df, given_time):\n",
    "    # Calculate absolute time differences\n",
    "    \n",
    "    time_diff = (df['end'] - given_time).abs()\n",
    "    \n",
    "    # Find the index of the minimum difference\n",
    "    closest_index = time_diff.idxmin()\n",
    "    return closest_index\n",
    "\n",
    "def add_text_to_diarization(diarization_df, transcriptions_df):\n",
    "\n",
    "    # New column for the combined text\n",
    "    diarization_df['text'] = ''\n",
    "\n",
    "    for index, row in diarization_df.iterrows():\n",
    "        # Find closest start and end times in df2\n",
    "        \n",
    "        closest_start_index = find_closest_start_time(transcriptions_df, row['start'])\n",
    "        closest_end_index = find_closest_end_time(transcriptions_df, row['end'])\n",
    "\n",
    "        # Extract all rows in-between these indices\n",
    "        if closest_start_index <= closest_end_index:\n",
    "            relevant_text = transcriptions_df.loc[closest_start_index:closest_end_index, 'text']\n",
    "        else:\n",
    "            relevant_text = transcriptions_df.loc[closest_end_index:closest_start_index, 'text']\n",
    "\n",
    "        # Combine the text and add to dataframe\n",
    "        combined_text = \"\".join(relevant_text)\n",
    "        diarization_df.at[index, 'text'] = combined_text\n",
    "\n",
    "    # dataframe now contains the combined text in the new 'text' column\n",
    "    return diarization_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get all podcasts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get speaker-tagged transcriptions for each dataframe, and save them in the datasets directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diarizations = \"diarized_audio/raw_diarizations\"\n",
    "\n",
    "directories = [\n",
    "    directory\n",
    "    for directory in os.listdir(diarizations) \n",
    "    if \"speaker_turns.csv\" in os.listdir(os.path.join(diarizations, directory))\n",
    "]\n",
    "\n",
    "for podcast_name in directories:\n",
    "    directory = os.path.join(diarizations, podcast_name)\n",
    "\n",
    "    diarization = pd.read_csv(f\"diarized_audio/raw_diarizations/{podcast_name}/speaker_turns.csv\")\n",
    "    transcription = pd.read_csv(f\"transcribed_audio/{podcast_name}_transcribed.csv\")\n",
    "\n",
    "    # Do a bit of data cleaning\n",
    "    transcription['start'] = pd.to_timedelta(transcription['start'], unit='s')\n",
    "    transcription['end'] = pd.to_timedelta(transcription['end'], unit='s')\n",
    "\n",
    "    diarization['start'] = pd.to_timedelta(diarization['start'])\n",
    "    diarization['end'] = pd.to_timedelta(diarization['end'])\n",
    "\n",
    "    all_transcriptions = \"\".join(transcription[\"text\"])\n",
    "\n",
    "    final_df = add_text_to_diarization(diarization, transcription)\n",
    "    final_df.to_csv(os.path.join(\"datasets\", podcast_name + \".csv\"), header=True, index=False)\n",
    "\n",
    "    # speaked = final_df.groupby(\"speaker\")['text'].apply(lambda x: ''.join(x))\n",
    "    # print(\"Number of words:\", len(tweet_tokenizer.tokenize(all_transcriptions)))\n",
    "    # print(\"Number of sentences:\", len(sent_tokenize(all_transcriptions)))\n",
    "    # print(speaked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then join them all in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = \"diarized_transcribed_df\"\n",
    "podcast_arr = []\n",
    "\n",
    "for podcast_name in directories:\n",
    "    podcast = pd.read_csv(os.path.join('datasets', podcast_name + '.csv'))\n",
    "    podcast_arr.append(podcast)\n",
    "\n",
    "diarized_transcribed_df = pd.concat(podcast_arr)\n",
    "diarized_transcribed_df.to_csv(os.path.join(\"datasets\", df_name + '.csv'), header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = diarized_transcribed_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the podcasts to ensure everything is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df['start'] = pd.to_timedelta(text_df['start'])\n",
    "text_df['end'] = pd.to_timedelta(text_df['end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_df = (\n",
    "    text_df[\n",
    "        ~(( text_df['end'] - text_df['start'] ) < pd.Timedelta(1, unit='s'))\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 107 duplicates\n"
     ]
    }
   ],
   "source": [
    "# Are there duplicates?\n",
    "print(f\"There are {fix_df['text'].duplicated().sum()} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "duplicates_across_speakers = fix_df[fix_df.duplicated(subset='text', keep=False)]\n",
    "\n",
    "unique_speakers_per_text = duplicates_across_speakers.drop_duplicates(subset=['text', 'speaker'])\n",
    "\n",
    "texts_to_remove = unique_speakers_per_text[unique_speakers_per_text.duplicated(subset='text', keep=False)]['text']\n",
    "\n",
    "# Remove entirely duplicate text if different speakers have said it\n",
    "fix_df = fix_df[~fix_df['text'].isin(texts_to_remove)]\n",
    "\n",
    "# Keep the first ocurrence of duplicate text if the same speaker has said it\n",
    "# (it could be a catch phrase, or something that identifies the speaker, which is very useful for speaker prediction)\n",
    "fix_df = fix_df.drop_duplicates(subset=['speaker', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that we have removed all the duplicates\n",
    "fix_df['text'].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_grouped_df = (\n",
    "    fix_df.groupby('speaker')['text']\n",
    "    .agg(\n",
    "        lambda x: ''.join(x)\n",
    "    ).to_frame().reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = text_df.groupby('speaker')['text'].agg(lambda x: ''.join(x)).to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df['words'] = grouped_df['text'].apply(lambda x: len(tweet_tokenizer.tokenize(x)))\n",
    "grouped_df['sentences']  = grouped_df['text'].apply(lambda x: len(sent_tokenize(x)))\n",
    "\n",
    "fix_grouped_df['words'] = fix_grouped_df['text'].apply(lambda x: len(tweet_tokenizer.tokenize(x)))\n",
    "fix_grouped_df['sentences']  = fix_grouped_df['text'].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eliminated Words = 34613 out of 444378 (7.79%) | 444378 reduced to -> 409765 \n",
      "Eliminated Sentences = 2937 out of 33511 (8.76%) | 33511 reduced to -> 30574 \n"
     ]
    }
   ],
   "source": [
    "wordiff = sum(grouped_df['words'] - fix_grouped_df['words'])\n",
    "sentdiff = sum(grouped_df['sentences'] - fix_grouped_df['sentences'])\n",
    "\n",
    "print(f\"Eliminated Words = {wordiff} out of {grouped_df['words'].sum()} ({((wordiff / grouped_df['words'].sum()) * 100):.2f}%) | {grouped_df['words'].sum()} reduced to -> {fix_grouped_df['words'].sum()} \")\n",
    "print(f\"Eliminated Sentences = {sentdiff} out of {grouped_df['sentences'].sum()} ({((sentdiff / grouped_df['sentences'].sum()) * 100):.2f}%) | {grouped_df['sentences'].sum()} reduced to -> {fix_grouped_df['sentences'].sum()} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>text</th>\n",
       "      <th>words</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ANDREW</td>\n",
       "      <td>Flatter for archaeologists. Yeah, it's from t...</td>\n",
       "      <td>27450</td>\n",
       "      <td>3317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANDREWHUBERMAN</td>\n",
       "      <td>Listen, when it comes to romantic relationshi...</td>\n",
       "      <td>49949</td>\n",
       "      <td>3605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BENSHAPIRO</td>\n",
       "      <td>The great light we tell ourselves is that peo...</td>\n",
       "      <td>28319</td>\n",
       "      <td>1621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GEORGEHOTZ</td>\n",
       "      <td>Sure. So I think the most obvious way to me i...</td>\n",
       "      <td>23863</td>\n",
       "      <td>2057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GUIDOVANROSSUM</td>\n",
       "      <td>How did you pull that off? And what's C Pytho...</td>\n",
       "      <td>23891</td>\n",
       "      <td>1214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HANCOCK</td>\n",
       "      <td>We're almost at the edge of history when we g...</td>\n",
       "      <td>26843</td>\n",
       "      <td>2027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KANYEWEST</td>\n",
       "      <td>Based off of our connection and just you bein...</td>\n",
       "      <td>21401</td>\n",
       "      <td>1594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LEXFRIDMAN</td>\n",
       "      <td>Can you imagine possible features that Python...</td>\n",
       "      <td>73199</td>\n",
       "      <td>5429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MARKZUCKERBERG</td>\n",
       "      <td>that experience like? Oh, it's fun. I know. Y...</td>\n",
       "      <td>35694</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MARQUES</td>\n",
       "      <td>Yeah. This is new to me. Yeah. No, you emaile...</td>\n",
       "      <td>15168</td>\n",
       "      <td>1355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MATTHEWMCCOUNAGHEY</td>\n",
       "      <td>If you really want to give a character an obs...</td>\n",
       "      <td>17624</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MRBEAST</td>\n",
       "      <td>I think maybe one of the videos we've already...</td>\n",
       "      <td>66364</td>\n",
       "      <td>6303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               speaker                                               text  \\\n",
       "0               ANDREW   Flatter for archaeologists. Yeah, it's from t...   \n",
       "1       ANDREWHUBERMAN   Listen, when it comes to romantic relationshi...   \n",
       "2           BENSHAPIRO   The great light we tell ourselves is that peo...   \n",
       "3           GEORGEHOTZ   Sure. So I think the most obvious way to me i...   \n",
       "4       GUIDOVANROSSUM   How did you pull that off? And what's C Pytho...   \n",
       "5              HANCOCK   We're almost at the edge of history when we g...   \n",
       "6            KANYEWEST   Based off of our connection and just you bein...   \n",
       "7           LEXFRIDMAN   Can you imagine possible features that Python...   \n",
       "8       MARKZUCKERBERG   that experience like? Oh, it's fun. I know. Y...   \n",
       "9              MARQUES   Yeah. This is new to me. Yeah. No, you emaile...   \n",
       "10  MATTHEWMCCOUNAGHEY   If you really want to give a character an obs...   \n",
       "11             MRBEAST   I think maybe one of the videos we've already...   \n",
       "\n",
       "    words  sentences  \n",
       "0   27450       3317  \n",
       "1   49949       3605  \n",
       "2   28319       1621  \n",
       "3   23863       2057  \n",
       "4   23891       1214  \n",
       "5   26843       2027  \n",
       "6   21401       1594  \n",
       "7   73199       5429  \n",
       "8   35694       1528  \n",
       "9   15168       1355  \n",
       "10  17624        524  \n",
       "11  66364       6303  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fix_grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter our df taking only speakers that have said more than 40k words\n",
    "speakers_of_interest = fix_grouped_df[fix_grouped_df['words'] > 40_000]['speaker']\n",
    "df = fix_df[fix_df['speaker'].isin(speakers_of_interest)][['speaker', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentences'] = df['text'].apply(sent_tokenize)\n",
    "df = df.explode('sentences').drop('text', axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only sentences with more than three words, others are irrelevant\n",
    "df['word_count'] = df['sentences'].apply(lambda x: len(x.split()))\n",
    "df = df[df['word_count'] > 3].drop('word_count', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LEXFRIDMAN</td>\n",
       "      <td>Can you imagine possible features that Python...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LEXFRIDMAN</td>\n",
       "      <td>of the new 4.0?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LEXFRIDMAN</td>\n",
       "      <td>Given the amount of pain and joy, suffering, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LEXFRIDMAN</td>\n",
       "      <td>his second time on this podcast.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LEXFRIDMAN</td>\n",
       "      <td>He is the creator of the Python programming la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16034</th>\n",
       "      <td>ANDREWHUBERMAN</td>\n",
       "      <td>I've seen so many people who were doing great,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16035</th>\n",
       "      <td>ANDREWHUBERMAN</td>\n",
       "      <td>So those would be the recommendations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16036</th>\n",
       "      <td>ANDREWHUBERMAN</td>\n",
       "      <td>There are a bunch of others and I'll keep spou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16040</th>\n",
       "      <td>ANDREWHUBERMAN</td>\n",
       "      <td>Oh, that's just Salmonella.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16042</th>\n",
       "      <td>ANDREWHUBERMAN</td>\n",
       "      <td>You're not in a lot of proctors making money ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12563 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              speaker                                          sentences\n",
       "0          LEXFRIDMAN   Can you imagine possible features that Python...\n",
       "1          LEXFRIDMAN                                    of the new 4.0?\n",
       "2          LEXFRIDMAN  Given the amount of pain and joy, suffering, a...\n",
       "3          LEXFRIDMAN                   his second time on this podcast.\n",
       "4          LEXFRIDMAN  He is the creator of the Python programming la...\n",
       "...               ...                                                ...\n",
       "16034  ANDREWHUBERMAN  I've seen so many people who were doing great,...\n",
       "16035  ANDREWHUBERMAN             So those would be the recommendations.\n",
       "16036  ANDREWHUBERMAN  There are a bunch of others and I'll keep spou...\n",
       "16040  ANDREWHUBERMAN                        Oh, that's just Salmonella.\n",
       "16042  ANDREWHUBERMAN   You're not in a lot of proctors making money ...\n",
       "\n",
       "[12563 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_and_split_dataframe(df, class_column, split_prob=[0.7, 0.15, 0.15]):\n",
    "\n",
    "    #split parameters\n",
    "    choices = ['train', 'test', 'val']\n",
    "\n",
    "    # Group by class and find the smallest class size\n",
    "    group = df.groupby(class_column)\n",
    "    smallest_class_size = group.size().min()\n",
    "\n",
    "    # Sample from each class (we can use with multi-label)\n",
    "    undersampled_df = pd.DataFrame()\n",
    "    for _, group_df in group:\n",
    "        sampled_df = group_df.sample(n=smallest_class_size, replace=False, random_state=1)\n",
    "\n",
    "        # Perform the split class by class, for the train, test and validation rows to be balanced.\n",
    "        sampled_df['split'] = np.random.choice(choices, size=len(sampled_df), p=split_prob)\n",
    "        undersampled_df = pd.concat([undersampled_df, sampled_df], axis=0)\n",
    "\n",
    "    # Check the distribution\n",
    "    print(undersampled_df['split'].value_counts(normalize=True))\n",
    "\n",
    "    return undersampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\n",
      "train    0.695123\n",
      "test     0.153796\n",
      "val      0.151081\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "prepared_df = undersample_and_split_dataframe(df, 'speaker')\n",
    "prepared_df.columns = ['category', 'title', 'split']\n",
    "prepared_df = prepared_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "ANDREWHUBERMAN    3069\n",
       "LEXFRIDMAN        3069\n",
       "MRBEAST           3069\n",
       "dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_df.groupby('category').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df.to_csv(os.path.join(\"datasets\", df_name + \"_preparado.csv\"), header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTANDWHISPER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
