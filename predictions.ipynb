{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.read_csv(os.path.join('datasets', 'JRE_kevinHart_25052020.csv'))\n",
    "speaker_series = full_df.groupby('speaker')['text'].apply(lambda x: ''.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COSINE SIMILARITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/guayo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/guayo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOEROGAN:  That's amazing. Framed in my fucking house. I hope Eddie comes back. I know he's been talking about it. He's been talking about it. Did you see that one thing that he did? There was some sort of an award show where he came up and talked on the podium and he was doing material and he was talking about them taking away Bill Cosby's degrees. No, you never seen it? No, Jamie. Find it. Find it.\n",
      "KEVINHART:  He knows where all these after hours places are. You go behind an alley, you knock on a door, they open it up.\n"
     ]
    }
   ],
   "source": [
    "#Get random sentences from both. Execute until satisfied with choice\n",
    "random_sent_joe = np.random.choice(full_df[full_df['speaker'] == 'JOEROGAN']['text'].to_list())\n",
    "random_sent_kevin = np.random.choice(full_df[full_df['speaker'] == 'KEVINHART']['text'].to_list())\n",
    "print(\"JOEROGAN:\", random_sent_joe)\n",
    "print(\"KEVINHART:\", random_sent_kevin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing (assuming the model was trained on lowercased text)\n",
    "def preprocess(text):\n",
    "    return [word for word in word_tokenize(text) if word.isalpha() and word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get similarity with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim import matutils\n",
    "\n",
    "# Load your pre-trained Word2Vec model\n",
    "model = Word2Vec.load('podcast_Word2Vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KEVIN HART SAID THIS\n",
      "KEVIN HART SAID THIS\n"
     ]
    }
   ],
   "source": [
    "# Convert text to vectors\n",
    "def text_to_vector(text):\n",
    "    words = preprocess(text)\n",
    "    word_vecs = [model.wv[word] for word in words if word in model.wv]\n",
    "    return np.mean(word_vecs, axis=0) if word_vecs else np.zeros(model.vector_size)\n",
    "\n",
    "corpus_joe = text_to_vector(speaker_series[\"JOEROGAN\"])\n",
    "corpus_kevin = text_to_vector(speaker_series[\"KEVINHART\"])\n",
    "vec_joe = text_to_vector(random_sent_joe)\n",
    "vec_kevin = text_to_vector(random_sent_kevin)\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# OUTPUT SHOULD BE --> JOE ROGAN SAID THIS\n",
    "corpus_joe_sim = cosine_similarity(corpus_joe.reshape(1, -1), vec_joe.reshape(1, -1))\n",
    "corpus_kevin_sim = cosine_similarity(corpus_kevin.reshape(1, -1), vec_joe.reshape(1, -1))\n",
    "\n",
    "if corpus_joe_sim > corpus_kevin_sim:\n",
    "    print('JOE ROGAN SAID THIS')\n",
    "else:\n",
    "    print('KEVIN HART SAID THIS')\n",
    "\n",
    "# OUTPUT SHOULD BE --> KEVIN HART SAID THIS\n",
    "corpus_joe_sim = cosine_similarity(corpus_joe.reshape(1, -1), vec_kevin.reshape(1, -1))\n",
    "corpus_kevin_sim = cosine_similarity(corpus_kevin.reshape(1, -1), vec_kevin.reshape(1, -1))\n",
    "\n",
    "if corpus_joe_sim > corpus_kevin_sim:\n",
    "    print('JOE ROGAN SAID THIS')\n",
    "else:\n",
    "    print('KEVIN HART SAID THIS')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the similarity with ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://allennlp.s3.amazonaws.com/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "\n",
    "elmo = Elmo(options_file, weight_file, 1, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float32' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/guayo/Desktop/programming/python/NLP/YT-WHISPER_SUMMARIZATION/predictions.ipynb Cell 11\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/guayo/Desktop/programming/python/NLP/YT-WHISPER_SUMMARIZATION/predictions.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m random_sent_joe_embedded \u001b[39m=\u001b[39m get_elmo_embeddings(random_sent_joe)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/guayo/Desktop/programming/python/NLP/YT-WHISPER_SUMMARIZATION/predictions.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m random_sent_kevin_embedded \u001b[39m=\u001b[39m get_elmo_embeddings(random_sent_kevin)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/guayo/Desktop/programming/python/NLP/YT-WHISPER_SUMMARIZATION/predictions.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m corpus_joe_embedded \u001b[39m=\u001b[39m get_elmo_embeddings(corpus_joe)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/guayo/Desktop/programming/python/NLP/YT-WHISPER_SUMMARIZATION/predictions.ipynb#X13sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m corpus_kevin_embedded \u001b[39m=\u001b[39m get_elmo_embeddings(corpus_kevin)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/guayo/Desktop/programming/python/NLP/YT-WHISPER_SUMMARIZATION/predictions.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maverage_embedding\u001b[39m(sentence_embedding):\n",
      "\u001b[1;32m/home/guayo/Desktop/programming/python/NLP/YT-WHISPER_SUMMARIZATION/predictions.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/guayo/Desktop/programming/python/NLP/YT-WHISPER_SUMMARIZATION/predictions.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_elmo_embeddings\u001b[39m(words):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/guayo/Desktop/programming/python/NLP/YT-WHISPER_SUMMARIZATION/predictions.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     character_ids \u001b[39m=\u001b[39m batch_to_ids(words)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/guayo/Desktop/programming/python/NLP/YT-WHISPER_SUMMARIZATION/predictions.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     embeddings \u001b[39m=\u001b[39m elmo(character_ids)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/guayo/Desktop/programming/python/NLP/YT-WHISPER_SUMMARIZATION/predictions.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings[\u001b[39m'\u001b[39m\u001b[39melmo_representations\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/NLPENV/lib/python3.10/site-packages/allennlp/modules/elmo.py:243\u001b[0m, in \u001b[0;36mbatch_to_ids\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    241\u001b[0m indexer \u001b[39m=\u001b[39m ELMoTokenCharactersIndexer()\n\u001b[1;32m    242\u001b[0m \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m batch:\n\u001b[0;32m--> 243\u001b[0m     tokens \u001b[39m=\u001b[39m [Token(token) \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m sentence]\n\u001b[1;32m    244\u001b[0m     field \u001b[39m=\u001b[39m TextField(tokens, {\u001b[39m\"\u001b[39m\u001b[39mcharacter_ids\u001b[39m\u001b[39m\"\u001b[39m: indexer})\n\u001b[1;32m    245\u001b[0m     instance \u001b[39m=\u001b[39m Instance({\u001b[39m\"\u001b[39m\u001b[39melmo\u001b[39m\u001b[39m\"\u001b[39m: field})\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float32' object is not iterable"
     ]
    }
   ],
   "source": [
    "def get_elmo_embeddings(words):\n",
    "    character_ids = batch_to_ids(words)\n",
    "    embeddings = elmo(character_ids)\n",
    "    return embeddings['elmo_representations'][0]\n",
    "\n",
    "random_sent_joe_embedded = get_elmo_embeddings(random_sent_joe)\n",
    "random_sent_kevin_embedded = get_elmo_embeddings(random_sent_kevin)\n",
    "corpus_joe_embedded = get_elmo_embeddings(corpus_joe)\n",
    "corpus_kevin_embedded = get_elmo_embeddings(corpus_kevin)\n",
    "\n",
    "def average_embedding(sentence_embedding):\n",
    "    return sentence_embedding.mean(dim=0)\n",
    "\n",
    "similarities = [cosine_similarity(corpus_joe, vec_joe), cosine_similarity(corpus_kevin, vec_kevin)]\n",
    "print(similarities)\n",
    "\n",
    "similarities[\n",
    "    cosine_similarity(average_embedding(corpus_joe_embedded), average_embedding(random_sent_joe_embedded), dim=0),\n",
    "    cosine_similarity(average_embedding(corpus_kevin_embedded), average_embedding(random_sent_joe_embedded), dim=0)\n",
    "]\n",
    "\n",
    "print(f\"La similitud del coseno entre las dos oraciones es: {[s.item() for s in similarities]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTANDWHISPER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
